{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4ff7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_blobs\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e49d5",
   "metadata": {},
   "source": [
    "We build a density model and when a new datapoint comes, we measure the probability of it that how likely is it to see this datapoint.\n",
    "<br><br>\n",
    "# Discrete Data Density Estimation\n",
    "Density Estimation has vast application, such as in understanding the structure of the multivariate data, finding outliers, estimating the likelihood of seeing a datapoint in the data and many more. \n",
    "First, we understand the Discrete Data Density Estimation. In case of multivariate categorical data, a column having three color categories viz Red, Blue and Green. Then the sum of individual color density should be 1. That is,<br>\n",
    "$P(R)+P(B)+P(G)=1$ \n",
    "<br>  \n",
    "Here we are learning three parameters that we need to estimate from the data, but the number of free parameters = Number of parameters  - Constraint ( probability adds up to 1) = 2.  In the mentioned case, we are estimating the density along each column i.e. one-dimensional density estimation.\n",
    "If we want to estimate joint estimation, for example, probability of red colored and triangular toy, then the probability would be:<br>\n",
    "$P(R, T)=\\frac{n(R, T)}{N}$\n",
    "<br>\n",
    "where N = total number of toys and n(R,T) = count of common observations where toy is red and triangular\n",
    "<br>\n",
    "In the joint space, how many parameters are we learning?<br>\n",
    "Let’s say that column “Color” takes 3  unique values and column “Type” takes 3 different values. Then the combined view has 9 cells i.e., number of parameters to learn= 9  and there is one constraint that sum of all possible groups is 1. So, we have 8 free parameters. <br><br>\n",
    "These are called Counting-based Density Estimation Functions. \n",
    "\n",
    "One important point to note:<br>\n",
    "If we increase the number of datapoints, counts or  number of observations increases per class, but the parameters(probability) of seeing a possible combination can still remain same. <br>\n",
    "![image](https://raw.githubusercontent.com/niteshjindal170988/unsupervised-learning/main/density-estimation /.scrap/1.JPG)<br>\n",
    "With more data, we get more confidence in the data however parameter value remains unchanged. As the data size increases, we tend to get more biased outcomes unlike prior estimates (i.e., unbiased estimates or equal probability estimates to each class).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ddd19",
   "metadata": {},
   "source": [
    "# Non Parameteric Density Estimation\n",
    "<br>\n",
    "Assume that each data point has a field of influence(i.e., a gaussian) and as we move away from the datapoint, the field of influence decreases. To calculate the density of any new point, we compute the distance of each datapoints from the new point which will provide us with the overall influence of all the points on the new point.Each gaussian has mean and variance and each gaussian sits on each datapoint. \n",
    "<br>\n",
    "Let $K_{\\sigma}\\left(x, x_{n}\\right)$ be the Kernel function where $x_{n}$ is the nth datapoint or the mean of the gaussian around nth datapoint. The bigger is distance between nth datapoint and datapoint “x”, smaller would be the overall influence i.e. smaller kernel function value. \n",
    "<br>\n",
    "The density of the point ‘x’ (i.e., the influence of all the datapoints on the datapoint ‘x’ ) is:<br>\n",
    "$P(x)=\\frac{1}{N} \\sum_{n=1}^{N} K_{\\sigma}\\left(x, x_{n}\\right)$\n",
    "<br><br> where <br>\n",
    "$K_{\\sigma}\\left(x, x_{n}\\right)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(x-x_{n}\\right)^{2}}{2 \\sigma^{2}}\\right)$\n",
    "<br><br> This is also known as Kernel Density Function or Parzen Window Function. Few points to know are:<br>\n",
    "-- In this process, we are not learning any parameters and just storing the datapoints. When a new datapoint comes, we estimate the probability of the new datapoint by using the given kernal function. That’s why this is called non-parameteric methods.<br>\n",
    "\n",
    "-- Only the scoring time is involved, but no training takes places therefore, there is no training time associated with non-parameteric approach. <br>\n",
    "-- $\\sigma$ is the hyperparameter and there are no parameters involved.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9369c7f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
