{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc695b2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/niteshjindal170988/unsupervised-learning/blob/main/clustering/k_means_clustering.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05d7b8",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5d1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_blobs\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ee1e9",
   "metadata": {},
   "source": [
    "In this exercise, we will follow the following steps:\n",
    "-  PCA on [Digit Recognizer Dataset](https://www.kaggle.com/c/digit-recognizer) to get the top-30 projections/dimensions that captures the maximum variance of  data.<br>\n",
    "- K Means Clustering on PCA output (i.e. 30 features data). Define 10 clusters to partition the data.<br>\n",
    "    - K Means Clustering on the subset of Data.<br>\n",
    "    - K Means Clustering on the entire Dataset.\n",
    "Details on PCA can be checked here [PCA-wikipage](https://en.wikipedia.org/wiki/Principal_component_analysis) and the notebook on PCA in the same repository- [PCA notebook](https://github.com/niteshjindal170988/unsupervised-learning/blob/main/dimensionality-reduction/principal_component_analysis_digit_recognizer.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bd85c",
   "metadata": {},
   "source": [
    "# PCA on the Digit-Recognizer Data\n",
    "\n",
    "## Download Dataset from Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7143b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown==4.2.0 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (4.2.0)\n",
      "Requirement already satisfied: requests[socks] in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from gdown==4.2.0) (2.28.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from gdown==4.2.0) (4.11.1)\n",
      "Requirement already satisfied: filelock in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from gdown==4.2.0) (3.7.1)\n",
      "Requirement already satisfied: tqdm in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from gdown==4.2.0) (4.64.0)\n",
      "Requirement already satisfied: six in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from gdown==4.2.0) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from beautifulsoup4->gdown==4.2.0) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from requests[socks]->gdown==4.2.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from requests[socks]->gdown==4.2.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from requests[socks]->gdown==4.2.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from requests[socks]->gdown==4.2.0) (2.0.12)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/nitesh/env/dev37/python37/lib/python3.7/site-packages (from requests[socks]->gdown==4.2.0) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/nitesh/env/dev37/python37/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1SfSO5ZloHH3W6GJa5rfy9-qwjG4YPbM4\n",
      "To: /home/nitesh/env/dev37/repos/unsupervised-learning/clustering/train.csv\n",
      "100%|██████████| 76.8M/76.8M [00:04<00:00, 16.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org gdown==4.2.0\n",
    "import gdown\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load digit-recognizer (Train Data)\n",
    "url = 'https://drive.google.com/uc?id=1SfSO5ZloHH3W6GJa5rfy9-qwjG4YPbM4'\n",
    "output = 'train.csv'\n",
    "gdown.download(url, output, quiet=False, verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef46e3",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca2adc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "display(data.head()) #  Digits / Pixel data\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abaef0f",
   "metadata": {},
   "source": [
    "## Remove installed file from the directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38438b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %pwd\n",
    "    %rm train.csv\n",
    "except:\n",
    "    !pwd\n",
    "    !rm train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d570d",
   "metadata": {},
   "source": [
    "The data set-`train.csv` has 785 columns. The first column, called `label`, which is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "There are 10 labels (0 to 9).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0be4db",
   "metadata": {},
   "source": [
    "# Define a Class PCA to get the Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b193ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    #lbl_col_index=0 # global variable ; column index/position having class labels.\n",
    "    def __init__(self, df, lbl_col_index=0):\n",
    "    \n",
    "        '''\n",
    "        Declare instance variables\n",
    "        '''\n",
    "        self.df=df  \n",
    "        self.lbl_col_ind=lbl_col_index\n",
    "        self.labels = pd.Series(self.df.iloc[:,self.lbl_col_ind]).unique() #unique target labels \n",
    "        self.labelseries = pd.Series(self.df.iloc[:,self.lbl_col_ind])\n",
    "#         print(self.labels)\n",
    "        \n",
    "    def drop_target_column(self, dataframe):\n",
    "        features= dataframe.drop(dataframe.iloc[:,self.lbl_col_ind:self.lbl_col_ind+1], axis=1)\n",
    "        return features\n",
    "            \n",
    "        \n",
    "        \n",
    "    def subset_data(self, uniq_trgt_lbl:int = None):\n",
    "        '''\n",
    "        Takes an integer value as an input (numeric category label).\n",
    "        Returns-\n",
    "        (1) Pandas Dataframe which is scaled subset of data without labels \n",
    "        (2) Pandas Series of Labels \n",
    "        Standard Scaler returns values with zero mean and unit variance.\n",
    "        '''\n",
    "        if uniq_trgt_lbl is None:\n",
    "            subdf = self.df\n",
    "        else:\n",
    "            subdf=self.df[self.df.iloc[:,self.lbl_col_ind] == self.labels[uniq_trgt_lbl]]\n",
    "            \n",
    "        catg = subdf.iloc[:,self.lbl_col_ind]\n",
    "        features = self.drop_target_column(subdf)\n",
    "        return features, catg\n",
    "    \n",
    "    \n",
    "    def feature_scaling(self, features):\n",
    "        scaled_features=StandardScaler(copy=True, with_mean=True).fit_transform(features)\n",
    "        return scaled_features\n",
    "        \n",
    "        \n",
    "    def cov_mat(self, uniq_trgt_lbl: int=None):\n",
    "        '''\n",
    "        Takes in the unique target label to filter the data.\n",
    "        Returns the scaled data of dimensions (4132, 784) \n",
    "        and the covariance matrix of scaled data of dimensions (784, 784).\n",
    "        '''\n",
    "        if uniq_trgt_lbl is None:\n",
    "            subdf=self.drop_target_column(self.df)\n",
    "            scaled_feat=self.feature_scaling(subdf)\n",
    "            covmat = np.cov(scaled_feat, rowvar = False, bias = False)\n",
    "            return scaled_feat, covmat\n",
    "        else:\n",
    "            subdf=self.subset_data(uniq_trgt_lbl)[0] #get the scaled features \n",
    "            scaled_feat=self.feature_scaling(subdf)\n",
    "            covmat = np.cov(scaled_feat, rowvar = False, bias = False)\n",
    "            return scaled_feat, covmat\n",
    "         \n",
    "\n",
    "    def eig_val_eig_vec(self, covariance_matrix):\n",
    "        \n",
    "        '''\n",
    "        Takes input as square array / covariance matrix\n",
    "        and returns pairs of eigen value and eigen vector of the\n",
    "        covariance matrix in descending value of eigen value.\n",
    "        '''\n",
    "        \n",
    "        eigval, eigvec = np.linalg.eig(covariance_matrix)\n",
    "        pairs_eigval_eigvec = [(np.abs(eigval[k]), eigvec[:,k]) for k in range(len(eigval))]\n",
    "        sorted_eg_ev_pairs = sorted(pairs_eigval_eigvec, key=lambda rw: rw[0], reverse=True)  \n",
    "        return sorted_eg_ev_pairs \n",
    "    \n",
    "    def visualize_explained_variance(self, covariance_matrix):\n",
    "        information=self.eig_val_eig_vec(covariance_matrix)\n",
    "        eigval= [i[0] for i in information] #array containing eigen values sorted in descending order\n",
    "        var_exp = [(i/sum(eigval)) for i in eigval] \n",
    "        cum_sum_exp = np.cumsum(var_exp) #cummulative explained variance \n",
    "    \n",
    "        plt.step(range(0,len(cum_sum_exp)),\n",
    "                 cum_sum_exp,\n",
    "                 where='mid',\n",
    "                 label='Cumulative explained variance')\n",
    "        \n",
    "        plt.ylabel('Explained variance ratio')\n",
    "        plt.xlabel('Principal component index')\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def extract_n_principal_components(self, covariance_matrix, reqno_of_pcs):\n",
    "        \n",
    "        '''\n",
    "        Horizontally stacks the top two eigen vectors ordered based on\n",
    "        descending eigen values\n",
    "        '''\n",
    "            \n",
    "        srtd_eg_ev_pair = self.eig_val_eig_vec(covariance_matrix)\n",
    "        stacked_cmpnts=[np.hstack((k[1].reshape(784,1))) for k in srtd_eg_ev_pair[0:reqno_of_pcs]]\n",
    "        stacked_arr=np.asarray(stacked_cmpnts).T\n",
    "        return stacked_arr\n",
    "    \n",
    "    def get_projected_data(self, scaleddata, covariance_matrix, reqno_of_pcs):\n",
    "        '''\n",
    "        Takes the covariance matrix of dimensions D*D \n",
    "        Computes the Dot Product of scaled data for cat0 -> (4132*784)\n",
    "        with the eigen vectors of Covariance Matrix  with highest eigen values (784*2)\n",
    "        Return the projected data set of dimensions (m*2) for example for cat0->(4132*2)\n",
    "        '''\n",
    "        stcked_cmpnts = self.extract_n_principal_components(covariance_matrix, reqno_of_pcs)\n",
    "        projecteddata=pd.DataFrame(scaleddata.dot(stcked_cmpnts))\n",
    "        projecteddata.columns = [\"PC_\" + str(col) for col in projecteddata.columns]\n",
    "        return projecteddata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13263a1",
   "metadata": {},
   "source": [
    "# Extract the Top-30 Principal Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ea4673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC_0</th>\n",
       "      <th>PC_1</th>\n",
       "      <th>PC_2</th>\n",
       "      <th>PC_3</th>\n",
       "      <th>PC_4</th>\n",
       "      <th>PC_5</th>\n",
       "      <th>PC_6</th>\n",
       "      <th>PC_7</th>\n",
       "      <th>PC_8</th>\n",
       "      <th>PC_9</th>\n",
       "      <th>...</th>\n",
       "      <th>PC_21</th>\n",
       "      <th>PC_22</th>\n",
       "      <th>PC_23</th>\n",
       "      <th>PC_24</th>\n",
       "      <th>PC_25</th>\n",
       "      <th>PC_26</th>\n",
       "      <th>PC_27</th>\n",
       "      <th>PC_28</th>\n",
       "      <th>PC_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.140478</td>\n",
       "      <td>-5.226445</td>\n",
       "      <td>3.887001</td>\n",
       "      <td>-0.901512</td>\n",
       "      <td>-4.929111</td>\n",
       "      <td>-2.035413</td>\n",
       "      <td>-4.706946</td>\n",
       "      <td>4.767184</td>\n",
       "      <td>-0.230958</td>\n",
       "      <td>-1.460962</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282251</td>\n",
       "      <td>1.390834</td>\n",
       "      <td>1.033241</td>\n",
       "      <td>-2.450662</td>\n",
       "      <td>-0.025323</td>\n",
       "      <td>-0.543821</td>\n",
       "      <td>-2.181736</td>\n",
       "      <td>1.114760</td>\n",
       "      <td>-0.497239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.292332</td>\n",
       "      <td>6.032996</td>\n",
       "      <td>1.308148</td>\n",
       "      <td>-2.383294</td>\n",
       "      <td>-3.095188</td>\n",
       "      <td>1.791095</td>\n",
       "      <td>3.772790</td>\n",
       "      <td>-0.153865</td>\n",
       "      <td>4.115192</td>\n",
       "      <td>-4.299357</td>\n",
       "      <td>...</td>\n",
       "      <td>3.397149</td>\n",
       "      <td>0.212285</td>\n",
       "      <td>-0.220914</td>\n",
       "      <td>0.623992</td>\n",
       "      <td>-4.191473</td>\n",
       "      <td>-0.001298</td>\n",
       "      <td>0.070117</td>\n",
       "      <td>-0.246701</td>\n",
       "      <td>-1.783818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.644503</td>\n",
       "      <td>-1.705813</td>\n",
       "      <td>2.289326</td>\n",
       "      <td>2.241135</td>\n",
       "      <td>-5.094426</td>\n",
       "      <td>4.152058</td>\n",
       "      <td>1.012004</td>\n",
       "      <td>-1.732559</td>\n",
       "      <td>-0.436261</td>\n",
       "      <td>-0.073687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925756</td>\n",
       "      <td>-2.307804</td>\n",
       "      <td>-1.882139</td>\n",
       "      <td>2.069934</td>\n",
       "      <td>-0.594148</td>\n",
       "      <td>-1.129816</td>\n",
       "      <td>-1.538711</td>\n",
       "      <td>0.583776</td>\n",
       "      <td>0.248477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.474207</td>\n",
       "      <td>5.836139</td>\n",
       "      <td>2.008617</td>\n",
       "      <td>4.271106</td>\n",
       "      <td>-2.377777</td>\n",
       "      <td>-2.179913</td>\n",
       "      <td>-4.398030</td>\n",
       "      <td>0.353712</td>\n",
       "      <td>-0.992308</td>\n",
       "      <td>5.501253</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.099471</td>\n",
       "      <td>-0.531728</td>\n",
       "      <td>-0.489333</td>\n",
       "      <td>-3.446940</td>\n",
       "      <td>-1.623493</td>\n",
       "      <td>-0.844744</td>\n",
       "      <td>0.505766</td>\n",
       "      <td>0.655911</td>\n",
       "      <td>0.779984</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.559574</td>\n",
       "      <td>6.024818</td>\n",
       "      <td>0.933179</td>\n",
       "      <td>-3.012645</td>\n",
       "      <td>-9.489179</td>\n",
       "      <td>2.331195</td>\n",
       "      <td>6.149597</td>\n",
       "      <td>1.783637</td>\n",
       "      <td>4.123302</td>\n",
       "      <td>-5.757361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.842493</td>\n",
       "      <td>0.868689</td>\n",
       "      <td>-0.167608</td>\n",
       "      <td>2.809954</td>\n",
       "      <td>-3.381034</td>\n",
       "      <td>-1.239311</td>\n",
       "      <td>0.041742</td>\n",
       "      <td>-1.488019</td>\n",
       "      <td>-0.685522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>13.678849</td>\n",
       "      <td>-1.350366</td>\n",
       "      <td>-3.957336</td>\n",
       "      <td>-5.379672</td>\n",
       "      <td>-10.875898</td>\n",
       "      <td>5.105523</td>\n",
       "      <td>-0.071920</td>\n",
       "      <td>5.084014</td>\n",
       "      <td>4.253677</td>\n",
       "      <td>-0.673734</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.519416</td>\n",
       "      <td>1.800787</td>\n",
       "      <td>2.581044</td>\n",
       "      <td>-0.835002</td>\n",
       "      <td>0.791062</td>\n",
       "      <td>1.092196</td>\n",
       "      <td>0.969723</td>\n",
       "      <td>-4.285741</td>\n",
       "      <td>-1.025209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>-8.869582</td>\n",
       "      <td>-1.187360</td>\n",
       "      <td>2.323167</td>\n",
       "      <td>1.528830</td>\n",
       "      <td>-5.798988</td>\n",
       "      <td>2.821950</td>\n",
       "      <td>0.351780</td>\n",
       "      <td>-0.529810</td>\n",
       "      <td>-0.992204</td>\n",
       "      <td>-1.126098</td>\n",
       "      <td>...</td>\n",
       "      <td>2.021420</td>\n",
       "      <td>-2.783751</td>\n",
       "      <td>-1.306372</td>\n",
       "      <td>1.845894</td>\n",
       "      <td>-0.297738</td>\n",
       "      <td>-0.217034</td>\n",
       "      <td>-0.036322</td>\n",
       "      <td>0.339123</td>\n",
       "      <td>1.303649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>0.495391</td>\n",
       "      <td>7.076277</td>\n",
       "      <td>-12.089700</td>\n",
       "      <td>-3.223278</td>\n",
       "      <td>-0.618203</td>\n",
       "      <td>-0.330449</td>\n",
       "      <td>2.128035</td>\n",
       "      <td>-10.535164</td>\n",
       "      <td>2.225962</td>\n",
       "      <td>-1.881028</td>\n",
       "      <td>...</td>\n",
       "      <td>2.868081</td>\n",
       "      <td>-0.034297</td>\n",
       "      <td>-2.166629</td>\n",
       "      <td>-3.650243</td>\n",
       "      <td>-3.068946</td>\n",
       "      <td>2.451997</td>\n",
       "      <td>-0.601931</td>\n",
       "      <td>-0.825128</td>\n",
       "      <td>-2.011762</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>2.307240</td>\n",
       "      <td>-4.344513</td>\n",
       "      <td>0.699848</td>\n",
       "      <td>10.011222</td>\n",
       "      <td>5.586478</td>\n",
       "      <td>5.494875</td>\n",
       "      <td>-0.189789</td>\n",
       "      <td>-5.450360</td>\n",
       "      <td>-2.181693</td>\n",
       "      <td>-1.767516</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.260995</td>\n",
       "      <td>2.777467</td>\n",
       "      <td>-0.797147</td>\n",
       "      <td>0.717561</td>\n",
       "      <td>-2.477200</td>\n",
       "      <td>1.701241</td>\n",
       "      <td>2.232109</td>\n",
       "      <td>-0.524232</td>\n",
       "      <td>2.044783</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>-4.807670</td>\n",
       "      <td>1.559121</td>\n",
       "      <td>-2.497936</td>\n",
       "      <td>2.218724</td>\n",
       "      <td>1.041887</td>\n",
       "      <td>-0.168182</td>\n",
       "      <td>-1.191325</td>\n",
       "      <td>3.285766</td>\n",
       "      <td>1.626590</td>\n",
       "      <td>-1.431324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363469</td>\n",
       "      <td>1.032882</td>\n",
       "      <td>1.583452</td>\n",
       "      <td>-0.378018</td>\n",
       "      <td>-0.631496</td>\n",
       "      <td>-2.531351</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>-0.871751</td>\n",
       "      <td>-0.898639</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PC_0      PC_1       PC_2       PC_3       PC_4      PC_5  \\\n",
       "0      -5.140478 -5.226445   3.887001  -0.901512  -4.929111 -2.035413   \n",
       "1      19.292332  6.032996   1.308148  -2.383294  -3.095188  1.791095   \n",
       "2      -7.644503 -1.705813   2.289326   2.241135  -5.094426  4.152058   \n",
       "3      -0.474207  5.836139   2.008617   4.271106  -2.377777 -2.179913   \n",
       "4      26.559574  6.024818   0.933179  -3.012645  -9.489179  2.331195   \n",
       "...          ...       ...        ...        ...        ...       ...   \n",
       "41995  13.678849 -1.350366  -3.957336  -5.379672 -10.875898  5.105523   \n",
       "41996  -8.869582 -1.187360   2.323167   1.528830  -5.798988  2.821950   \n",
       "41997   0.495391  7.076277 -12.089700  -3.223278  -0.618203 -0.330449   \n",
       "41998   2.307240 -4.344513   0.699848  10.011222   5.586478  5.494875   \n",
       "41999  -4.807670  1.559121  -2.497936   2.218724   1.041887 -0.168182   \n",
       "\n",
       "           PC_6       PC_7      PC_8      PC_9  ...     PC_21     PC_22  \\\n",
       "0     -4.706946   4.767184 -0.230958 -1.460962  ...  1.282251  1.390834   \n",
       "1      3.772790  -0.153865  4.115192 -4.299357  ...  3.397149  0.212285   \n",
       "2      1.012004  -1.732559 -0.436261 -0.073687  ...  0.925756 -2.307804   \n",
       "3     -4.398030   0.353712 -0.992308  5.501253  ... -1.099471 -0.531728   \n",
       "4      6.149597   1.783637  4.123302 -5.757361  ... -0.842493  0.868689   \n",
       "...         ...        ...       ...       ...  ...       ...       ...   \n",
       "41995 -0.071920   5.084014  4.253677 -0.673734  ... -1.519416  1.800787   \n",
       "41996  0.351780  -0.529810 -0.992204 -1.126098  ...  2.021420 -2.783751   \n",
       "41997  2.128035 -10.535164  2.225962 -1.881028  ...  2.868081 -0.034297   \n",
       "41998 -0.189789  -5.450360 -2.181693 -1.767516  ... -1.260995  2.777467   \n",
       "41999 -1.191325   3.285766  1.626590 -1.431324  ...  0.363469  1.032882   \n",
       "\n",
       "          PC_23     PC_24     PC_25     PC_26     PC_27     PC_28     PC_29  \\\n",
       "0      1.033241 -2.450662 -0.025323 -0.543821 -2.181736  1.114760 -0.497239   \n",
       "1     -0.220914  0.623992 -4.191473 -0.001298  0.070117 -0.246701 -1.783818   \n",
       "2     -1.882139  2.069934 -0.594148 -1.129816 -1.538711  0.583776  0.248477   \n",
       "3     -0.489333 -3.446940 -1.623493 -0.844744  0.505766  0.655911  0.779984   \n",
       "4     -0.167608  2.809954 -3.381034 -1.239311  0.041742 -1.488019 -0.685522   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41995  2.581044 -0.835002  0.791062  1.092196  0.969723 -4.285741 -1.025209   \n",
       "41996 -1.306372  1.845894 -0.297738 -0.217034 -0.036322  0.339123  1.303649   \n",
       "41997 -2.166629 -3.650243 -3.068946  2.451997 -0.601931 -0.825128 -2.011762   \n",
       "41998 -0.797147  0.717561 -2.477200  1.701241  2.232109 -0.524232  2.044783   \n",
       "41999  1.583452 -0.378018 -0.631496 -2.531351  0.536819 -0.871751 -0.898639   \n",
       "\n",
       "       label  \n",
       "0          1  \n",
       "1          0  \n",
       "2          1  \n",
       "3          4  \n",
       "4          0  \n",
       "...      ...  \n",
       "41995      0  \n",
       "41996      1  \n",
       "41997      7  \n",
       "41998      6  \n",
       "41999      9  \n",
       "\n",
       "[42000 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uniq_categories=sorted(data.label.unique())\n",
    "#create PCA instance (object)\n",
    "pca_instance=PCA(data, 0)  ## 0 is the column index of labels in the data.\n",
    "scaled_dat, covmatr=pca_instance.cov_mat()\n",
    "PCA_30=pca_instance.get_projected_data(scaled_dat, covmatr, 30)\n",
    "PCA_30['label'] = pca_instance.labelseries\n",
    "PCA_30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95d272",
   "metadata": {},
   "source": [
    "<br><br>Clustering is the grouping of similar things. There are various clustering methods, for example-\n",
    "<br>\n",
    "![image](https://raw.githubusercontent.com/niteshjindal170988/unsupervised-learning/main/clustering/.scrap/clus0.JPG)\n",
    "<br>\n",
    "**K means clustering** is a type of Partitional Clustering. Let us say that we have n different data points $x_{1}, x_{2} \\ldots x_{n}$ and K different clusters, such as   $\\mathrm{m}_{1}, \\mathrm{~m}_{2} \\ldots \\mathrm{m}_{\\mathrm{k}}$. For every combination of data point and a cluster, we can compute a distance between them, for example the distance between $x_1$ and $m_1$ or distance between $x_1$ and $m_k$ and so on.\n",
    "Minimizing the distance between nth data point and kth cluster can be written as:\n",
    "$\\left(x_n-m_k\\right)^2$                           \n",
    "<br>\n",
    "However, every data point is not associated with every cluster. Thus, we need to associate the nth data point with the cluster it belongs to, not all the clusters. We will define a latent variable $\\delta_{n,k}$ that tells us whether the nth datapoint is associated with cluster K or not. $\\delta_{n,k}$ either 0 or 1. If it is 1, then the distance between the nth datapoint and kth cluster matters else not. <br><br>\n",
    "Therefore, we write the objective function as:\n",
    "<br><br>\n",
    "$J(m)=\\sum_{n} \\sum_{k} \\delta_{n, k}\\left(\\left\\|x_{n}-m_{k}\\right\\|\\right)^{2}$\n",
    "<br>\n",
    "<br>\n",
    "**Params**\t**Term**\t**Definition**<br><br>\n",
    "K\t= Hyperparameter\t= Number of Clusters <br><br>\n",
    "$m_k$\t= Parameters\t= Cluster Mean Vectors.<br><br>\n",
    "$D\\ast k$\t= Total Number of Parameters.\tEach $m_k$ is a D-dimensional vector and hence its matrix size is $D\\ast k$. Thus, the learned parameters are $D\\ast k$<br><br>\n",
    "$\\delta_{n,k}$\t= Latent Parameter<br><br>\n",
    "$\\delta_{n,k}$ that have value of 1 when there is an association between nth datapoint and a particular cluster. For all other combinations, the value would be 0. Latent parameter are similar to parameters, but we need them so as to write the objective function in a simple way.<br>\n",
    "<br> \n",
    "Hyperparameter tell us the complexity of the modelling system.\n",
    "<br><br>\n",
    "What makes $\\delta_{n,k}$ a latent parameter?\n",
    "<br>\n",
    "We learned that the size of $\\delta_{n,k}$ is  $n\\ast k$ and size of $m_k$ is $D\\ast k$ . If we know $\\delta_{n,k}$ , we can compute $m_k$ because we know that each datapoint is associated with a particular cluster only, and thus we can obtain the all the datapoints belonging to a particular cluster and take their mean to get cluster mean vector or cluster centers. So, the latent parameters are equivalent to parameters ( $m_k$ )\n",
    "<br>\n",
    "We can alternate between the $\\delta_{n,k}$ and  ${\\bar{m}}_k$  ( i.e. $\\delta_{n,k}$ is given and known to us and we compute ${\\bar{m}}_k$ , or ${\\bar{m}}_k$ is given and we compute $\\delta_{n,k}$).  In the following lines, we will consider $\\delta_{n,k}$ as constant and update the ${\\bar{m}}_k$ -\n",
    "<br><br>\n",
    "Steps:<br>\n",
    "\n",
    "-1 Cluster Assignment: Randomly Initialize The Cluster Centers\n",
    "<br>\n",
    "![image](https://raw.githubusercontent.com/niteshjindal170988/unsupervised-learning/main/clustering/.scrap/clus1.JPG)\n",
    "<br>\n",
    "-2 Compute the association of each Datapoint with each Cluster center (Euclidean distance). If distance of a datapoint is nearest to a particular cluster, map the datapoint to that cluster and assign $\\delta_{n,k}$ = 1 <br><br>\n",
    "-3 Recompute the Cluster Centers.\n",
    "<br>\n",
    "![image](https://raw.githubusercontent.com/niteshjindal170988/unsupervised-learning/main/clustering/.scrap/clus2.JPG)\n",
    "<br>\n",
    "We keep altering between the step 2 and step 3 until the convergence is met. \n",
    "<br><br>\n",
    "Let us derive the Cluster Centers:<br><br>\n",
    "$J(m, \\delta)=\\sum_{n=1}^{N} \\sum_{k=1}^{k} \\delta_{n, k}\\left(\\bar{m}_{k}-\\bar{x}_{n}\\right)^{2}$\n",
    "<br><br>Keep  $\\delta_{n,k}$ as constant and update the ${\\bar{m}}_j$. We take the partial derivative w.r.t ${\\bar{m}}_j$ (i.e., jth cluster):<br><br>\n",
    "=> $\\frac{\\partial J}{\\partial m_{j}}=2 \\sum_{n=1}^{N} \\delta_{n,j}\\left(\\bar{m}_{j}-\\bar{x}_{n}\\right)=0$<br><br>\n",
    "=> $\\sum_{n=1}^{N} \\delta_{n,j} \\cdot m_{j}$ = $\\sum_{n=1}^{N} \\delta_{n,j} \\bar{x}_{n}$<br><br>\n",
    "=> $\\mathbf{m}_{k}^{(t+1)} = \\frac{\\sum_{n=1}^{N} \\delta_{n, k}^{(t)} \\mathbf{X}^{n}}{\\sum_{n=1}^{N} \\delta_{n, k}^{(t)}}$\n",
    "\n",
    "<br><br>\n",
    "Output of k means clustering is a set of mean vectors. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d0ed3",
   "metadata": {},
   "source": [
    "# Let's get started ! \n",
    "\n",
    "## K-means Clustering on the PCA_30\n",
    "<br>\n",
    "We initialize the random clusters initially and then **iterate** of the datapoints and number of clusters until the convergence is achieved.Therefore, K Means Clustering is an **Iterative Method** as opposed to Closed Form of Solution Method, and is **Non-Deterministic**  as opposed to Deterministic, because the output depends on the initialization of random clusters in the K-means clustering.  Farthest first point initialization with K means clusterings makes the K-means Clustering somewhat deterministic, but if we do randomly initialize first point or density based sampling gives random or non-deterministic output. \n",
    "<br><br>\n",
    "Minimum number of clusters can be one (i.e. all datapoints in a single cluster) and maximum number of clusters can be equal to the number of datapoints in the data. As we **increase the complexity** i.e., the number of clusters,there would be an **increase in accuracy**. Model **memorizes** When the number of cluster becomes equal to number of datapoints, the accuracy is 100%. <br><br>\n",
    "\n",
    "<br><br>**In the following exercise, we will build 10 clusters and examine the effect of initialization on clusters-** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029dfb4",
   "metadata": {},
   "source": [
    "# Define kmeans() \n",
    "\n",
    "Apply k means clustering on batch of data. Each batch is a subset of data filtered with class label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d9168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(K, D: int, N: int, batch, cls_labelname):\n",
    "    '''\n",
    "    K is the number of clusters, D and N are batch dimensions and rows. \n",
    "    Batch is a subset of the data filtered with class label.\n",
    "    '''\n",
    "    \n",
    "    cluster_centers_T = np.zeros(shape=(D, K))  \n",
    "\n",
    "    #generate random vectors\n",
    "    for num in range(0, batch.shape[1]):\n",
    "        random.seed(num)\n",
    "        rv= random.sample(range(int(np.mean(batch) - 4.5*np.std(batch)), int(np.mean(batch) + 5.5*np.std(batch))), K)\n",
    "        cluster_centers_T[num] = rv\n",
    "\n",
    "    m_k = np.transpose(cluster_centers_T) # each of the 10 cluster centers is a 30 - dimensional vector.\n",
    "    m_k # cluster centers shape is (K * D) \n",
    "    m_k_initialization_matrix = np.zeros(m_k.shape) #  shape is (K*D) \n",
    "\n",
    "    #The linalg norm() function returns the norm of the given matrix or vector.\n",
    "    #Matrix norm measures the size of a matrix. \n",
    "    err = np.linalg.norm(m_k- m_k_initialization_matrix) \n",
    "    print(\"\\nError during first farther point initialization is {}\\n\".format(err))\n",
    "    iters_to_converge = 0\n",
    "    while err!=0:    \n",
    "        # dictionary contains information about best cluster corresponding to a datapoint in batch\n",
    "        datapoint_cluster_mapping_dict= dict() \n",
    "        delta_n_k = np.zeros(shape=(N, K)) #shape is (N*K)\n",
    "        for rw in range(len(batch)):\n",
    "              #get the euclidean distance between each batch row and all the k=10 cluster centers. \n",
    "            delta_n_k[rw] = np.linalg.norm(x = batch[rw] - m_k, keepdims = False,  axis=1) # (10,)\n",
    "            datapoint_cluster_mapping_dict[rw] = np.argmin(delta_n_k[rw]) \n",
    "        datapoint_cluster_mapping_dict  \n",
    "\n",
    "        m_k_initialization_matrix = deepcopy(m_k)\n",
    "\n",
    "        for cluster in range(0, K): #seggregate the datapoints within each cluster. \n",
    "            cluster_density = [batch[row] for row in range(len(batch)) if datapoint_cluster_mapping_dict[row] == cluster]\n",
    "            m_k[cluster] = np.nan_to_num(np.mean(np.array(cluster_density), axis=0)) # take mean of datapoints in cluster\n",
    "        err = np.linalg.norm(m_k - m_k_initialization_matrix)\n",
    "        if err <= 0.01:\n",
    "            iters_to_converge += 1\n",
    "            #print(\"\\nError in iteration {} is {}\\n\".format(iters_to_converge, err))\n",
    "            break;\n",
    "        else:\n",
    "            iters_to_converge += 1\n",
    "            #print(\"\\nError in iteration {} is {}\".format(iters_to_converge, err))\n",
    "        \n",
    "    print(\"\\nError in iteration {} is {}\\n\".format(iters_to_converge, err))\n",
    "    if cls_labelname is None:\n",
    "        data_cluster_mapping=datapoint_cluster_mapping_dict\n",
    "        return data_cluster_mapping\n",
    "    \n",
    "    else:\n",
    "        inv_datapoint_cluster_mapping_dict =sorted(\n",
    "          Counter(datapoint_cluster_mapping_dict.values()).items())\n",
    "        \n",
    "        output=pd.DataFrame(inv_datapoint_cluster_mapping_dict, \n",
    "                                columns = (\"cluster_number\", \"datapoints_cnt\")\n",
    "                                    ).assign(iters_to_converge=(iters_to_converge)\n",
    "                                            ).assign(class_label=cls_labelname)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9fa3d",
   "metadata": {},
   "source": [
    "# K Means Clustering on entire Dataset PCA_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543b9d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the Batch is  (42000, 30)\n",
      "\n",
      "Cluster  5\n",
      "\n",
      "Error during first farther point initialization is 109.69503179269333\n",
      "\n",
      "\n",
      "Error in iteration 106 is 0.006273639529707155\n",
      "\n",
      "Purity of Cluster 5 is 0.4101904761904762 \n",
      "\n",
      "\n",
      "Cluster  10\n",
      "\n",
      "Error during first farther point initialization is 164.76650144977893\n",
      "\n",
      "\n",
      "Error in iteration 115 is 0.005615580969186597\n",
      "\n",
      "Purity of Cluster 10 is 0.4804761904761905 \n",
      "\n",
      "\n",
      "Cluster  15\n",
      "\n",
      "Error during first farther point initialization is 205.64046294443125\n",
      "\n",
      "\n",
      "Error in iteration 110 is 0.0030026272976595996\n",
      "\n",
      "Purity of Cluster 15 is 0.5608095238095238 \n",
      "\n",
      "\n",
      "Cluster  20\n",
      "\n",
      "Error during first farther point initialization is 233.18447632722038\n",
      "\n",
      "\n",
      "Error in iteration 146 is 0.0\n",
      "\n",
      "Purity of Cluster 20 is 0.501 \n",
      "\n",
      "\n",
      "Cluster  25\n",
      "\n",
      "Error during first farther point initialization is 256.81900241220467\n",
      "\n",
      "\n",
      "Error in iteration 53 is 0.006220188655284142\n",
      "\n",
      "Purity of Cluster 25 is 0.5750952380952381 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Cluster Number'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeansobj=PCA(PCA_30, 30)  # label column position in PCA_30 is 30th\n",
    "unq_labels = kmeansobj.labels # 10 unique labels in the digits data\n",
    "f, c = kmeansobj.subset_data(None) \n",
    "\n",
    "batch=f.values\n",
    "D=batch.shape[1]\n",
    "N=batch.shape[0]\n",
    "print(\"\\nShape of the Batch is \", (N,D))\n",
    "Kluster=[5,10,15,20,25]\n",
    "cluster_quality_purity = []\n",
    "for K in Kluster: #range(15, 16):\n",
    "    print(\"\\nCluster \", K)\n",
    "    outp_d2c_mapping=kmeans(K, D, N, batch, None)\n",
    "    outp_mapping_dict=[(outp_d2c_mapping[obs_ind], obs_ind, c[obs_ind])\n",
    "                       for obs_ind in outp_d2c_mapping]\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "    dat=pd.DataFrame(outp_mapping_dict, columns = [\"cluster\", \"observation\", \"actual_label\"])\n",
    "    cls_cnt = dat.groupby(['cluster', 'actual_label']).count()['observation']\n",
    "    tot_cnts_all_cluster = 0\n",
    "    max_cnts_per_cluster=0\n",
    "    for k in range(K):\n",
    "        #print(np.argmax(cls_cnt[k]))  # get the class with maximum counts in a cluster\n",
    "        tot_cnts_all_cluster = tot_cnts_all_cluster + np.sum(cls_cnt[k])\n",
    "        try:\n",
    "            max_cnts_per_cluster = max_cnts_per_cluster + cls_cnt[k][np.argmax(cls_cnt[k])]\n",
    "        except KeyError:\n",
    "            max_cnts_per_cluster = max_cnts_per_cluster + cls_cnt[k][np.argmax(cls_cnt[k])+1]\n",
    "\n",
    "    purity = (max_cnts_per_cluster/tot_cnts_all_cluster)\n",
    "    cluster_quality_purity.append(purity)\n",
    "    print(\"Purity of Cluster {} is {} \\n\".format(K, purity))\n",
    "    \n",
    "    \n",
    "'''Plot Cluster vs Purity:- '''\n",
    "Kluster=list([5,10,15,20,25])\n",
    "cluster_eval=pd.DataFrame(zip(Kluster, cluster_quality_purity),\n",
    "                         columns =['Cluster Number', 'Cluster Quality(Purity)'])\n",
    "cluster_eval.plot(x='Cluster Number', y='Cluster Quality(Purity)', kind='line')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99507b",
   "metadata": {},
   "source": [
    "<br>we see the increase in purity with the increase in number of cluster centers.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddc5b7",
   "metadata": {},
   "source": [
    "# K Means Clustering on subsets of Dataset PCA_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f0c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K means on Sample with class label 1\n",
      "\n",
      "Error during first farther point initialization is 186.95721435665433\n",
      "\n",
      "\n",
      "Error in iteration 62 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 0\n",
      "\n",
      "Error during first farther point initialization is 121.33012816279393\n",
      "\n",
      "\n",
      "Error in iteration 44 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 4\n",
      "\n",
      "Error during first farther point initialization is 165.10905486980417\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 7\n",
      "\n",
      "Error during first farther point initialization is 147.91889669680478\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 3\n",
      "\n",
      "Error during first farther point initialization is 169.4609099468075\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 5\n",
      "\n",
      "Error during first farther point initialization is 151.62453627299243\n",
      "\n",
      "\n",
      "Error in iteration 58 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 8\n",
      "\n",
      "Error during first farther point initialization is 182.71562604221896\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 9\n",
      "\n",
      "Error during first farther point initialization is 168.28250057566888\n",
      "\n",
      "\n",
      "Error in iteration 46 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 2\n",
      "\n",
      "Error during first farther point initialization is 138.44854639901425\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 6\n",
      "\n",
      "Error during first farther point initialization is 138.44854639901425\n",
      "\n",
      "\n",
      "Error in iteration 43 is 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>datapoints_cnt</th>\n",
       "      <th>iters_to_converge</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>641</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>594</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>531</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>416</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>246</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>363</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>351</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>318</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>459</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>513</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>870</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>649</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>591</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>191</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>538</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>804</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>558</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>610</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>332</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>895</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>283</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>464</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>660</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>198</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>169</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>908</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>248</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>129</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>971</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>383</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>615</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>682</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>863</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>861</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>294</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>201</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>276</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>906</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>346</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>446</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>369</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>526</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>376</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>327</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>310</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>491</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>123</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>349</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>550</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>462</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>947</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>281</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>254</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>734</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>316</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>530</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>873</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>425</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>856</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>412</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>133</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>524</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>283</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>486</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>568</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>309</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>606</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>142</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>870</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>648</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>378</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>814</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>228</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>142</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>624</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>590</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>325</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>198</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_number  datapoints_cnt  iters_to_converge  class_label\n",
       "0       0               0             472                 62            1\n",
       "1       1               1             641                 62            1\n",
       "2       2               2             594                 62            1\n",
       "3       3               3             531                 62            1\n",
       "4       4               4             416                 62            1\n",
       "5       5               5             246                 62            1\n",
       "6       6               6             200                 62            1\n",
       "7       7               7             363                 62            1\n",
       "8       8               8             351                 62            1\n",
       "9       9               9             318                 62            1\n",
       "10      0               0             459                 44            0\n",
       "11      1               1             513                 44            0\n",
       "12      2               2             870                 44            0\n",
       "13      3               3             649                 44            0\n",
       "14      4               4              62                 44            0\n",
       "15      5               5             591                 44            0\n",
       "16      6               6             191                 44            0\n",
       "17      7               7             538                 44            0\n",
       "18      8               8               7                 44            0\n",
       "19      9               9             804                 44            0\n",
       "20      0               0             558                 38            4\n",
       "21      1               1             610                 38            4\n",
       "22      2               2             332                 38            4\n",
       "23      3               3             895                 38            4\n",
       "24      4               4             283                 38            4\n",
       "25      5               5             464                 38            4\n",
       "26      6               6             660                 38            4\n",
       "27      7               7             198                 38            4\n",
       "28      8               8             182                 38            4\n",
       "29      9               9             169                 38            4\n",
       "30      0               0             908                 42            7\n",
       "31      1               1             660                 42            7\n",
       "32      2               2             165                 42            7\n",
       "33      3               3             248                 42            7\n",
       "34      4               4             129                 42            7\n",
       "35      5               5             971                 42            7\n",
       "36      6               6             383                 42            7\n",
       "37      7               7              64                 42            7\n",
       "38      8               8              45                 42            7\n",
       "39      9               9             615                 42            7\n",
       "40      0               0             102                 42            3\n",
       "41      1               1             682                 42            3\n",
       "42      2               2             863                 42            3\n",
       "43      3               3             861                 42            3\n",
       "44      4               4             160                 42            3\n",
       "45      5               5              56                 42            3\n",
       "46      6               6             294                 42            3\n",
       "47      7               7             201                 42            3\n",
       "48      8               8             276                 42            3\n",
       "49      9               9             906                 42            3\n",
       "50      0               0             346                 58            5\n",
       "51      1               1             481                 58            5\n",
       "52      2               2             446                 58            5\n",
       "53      3               3             369                 58            5\n",
       "54      4               4             526                 58            5\n",
       "55      5               5             376                 58            5\n",
       "56      6               6             327                 58            5\n",
       "57      7               7             310                 58            5\n",
       "58      8               8             491                 58            5\n",
       "59      9               9             123                 58            5\n",
       "60      0               0             245                 39            8\n",
       "61      1               1             349                 39            8\n",
       "62      2               2              39                 39            8\n",
       "63      3               3             550                 39            8\n",
       "64      4               4             462                 39            8\n",
       "65      5               5             947                 39            8\n",
       "66      6               6             281                 39            8\n",
       "67      7               7             254                 39            8\n",
       "68      8               8             734                 39            8\n",
       "69      9               9             316                 39            8\n",
       "70      0               0             182                 46            9\n",
       "71      1               1             530                 46            9\n",
       "72      2               2             873                 46            9\n",
       "73      3               3             161                 46            9\n",
       "74      4               4             425                 46            9\n",
       "75      5               5             856                 46            9\n",
       "76      6               6             412                 46            9\n",
       "77      7               7             133                 46            9\n",
       "78      8               8             524                 46            9\n",
       "79      9               9              41                 46            9\n",
       "80      0               0             750                 30            2\n",
       "81      1               1             283                 30            2\n",
       "82      2               2              57                 30            2\n",
       "83      3               3             486                 30            2\n",
       "84      4               4             568                 30            2\n",
       "85      5               5             309                 30            2\n",
       "86      6               6             606                 30            2\n",
       "87      7               7             142                 30            2\n",
       "88      8               8             870                 30            2\n",
       "89      9               9               1                 30            2\n",
       "90      0               0             648                 43            6\n",
       "91      1               1             378                 43            6\n",
       "92      2               2             814                 43            6\n",
       "93      3               3             228                 43            6\n",
       "94      4               4             142                 43            6\n",
       "95      5               5             624                 43            6\n",
       "96      6               6             590                 43            6\n",
       "97      7               7             116                 43            6\n",
       "98      8               8             325                 43            6\n",
       "99      9               9             198                 43            6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create kmeansobj instance \n",
    "kmeansobj=PCA(PCA_30, 30)  # label column position in PCA_30 is 30th\n",
    "unq_labels = kmeansobj.labels # 10 unique labels in the digits data\n",
    "\n",
    "# #iteration on class label \n",
    "# subclass, cls_label = kmeansobj.subset_data(0)  #subset data with label = 0 \n",
    "# batch = subclass.values # batch has shape N*30. dimensions = 30 (after PCA).\n",
    "\n",
    "#define global parameters\n",
    "K = 10 # number of clusters as per user requirement (user defined)\n",
    "# D = batch.shape[1]  # dimensions = 30 \n",
    "# N = batch.shape[0]  # sample datapoints = N \n",
    "output_summary = pd.DataFrame()\n",
    "for cls_labelname in unq_labels:\n",
    "    print(\"\\nK means on Sample with class label {}\".format(cls_labelname))\n",
    "    subclass, cls_label = kmeansobj.subset_data(cls_labelname)  #subset data with label = 0 \n",
    "    batch = subclass.values # batch has shape N*30. dimensions = 30 (after PCA).\n",
    "    D = batch.shape[1]  # dimensions = 30 \n",
    "    N = batch.shape[0]  # sample datapoints = N \n",
    "    outp=kmeans(K, D, N, batch, cls_labelname)\n",
    "    output_summary=pd.concat([output_summary,outp],axis=0)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "output_summary=output_summary.reset_index()\n",
    "\n",
    "output_summary.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6bd69a2e491e1c24bfa82aac28742e1679925a6038dda4b24aaf6fcc9809d2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
