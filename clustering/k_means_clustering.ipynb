{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc695b2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/niteshjindal170988/unsupervised-learning/blob/main/clustering/k_means_clustering.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05d7b8",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5d1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_blobs\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ee1e9",
   "metadata": {},
   "source": [
    "In this exercise, we will follow the following steps:\n",
    "-  PCA on [Digit Recognizer Dataset](https://www.kaggle.com/c/digit-recognizer) to get the top-30 projections/dimensions that captures the maximum variance of  data.<br>\n",
    "- K Means Clustering on PCA output (i.e. 30 features data). Define 10 clusters to partition the data.<br>\n",
    "    - K Means Clustering on the subset of Data.<br>\n",
    "    - K Means Clustering on the entire Dataset.\n",
    "Details on PCA can be checked here [PCA-wikipage](https://en.wikipedia.org/wiki/Principal_component_analysis) and the notebook on PCA in the same repository- [PCA notebook](https://github.com/niteshjindal170988/unsupervised-learning/blob/main/dimensionality-reduction/principal_component_analysis_digit_recognizer.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bd85c",
   "metadata": {},
   "source": [
    "# PCA on the Digit-Recognizer Data\n",
    "\n",
    "## Download Dataset from Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.2; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown==4.2.0 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (4.62.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (4.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from beautifulsoup4->gdown==4.2.0) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from tqdm->gdown==4.2.0) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1SfSO5ZloHH3W6GJa5rfy9-qwjG4YPbM4\n",
      "To: C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\tutorials\\unsupervised-learning\\clustering\\train.csv\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▌   | 73.4M/76.8M [01:34<00:04, 755kB/s]"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org gdown==4.2.0\n",
    "import gdown\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load digit-recognizer (Train Data)\n",
    "url = 'https://drive.google.com/uc?id=1SfSO5ZloHH3W6GJa5rfy9-qwjG4YPbM4'\n",
    "output = 'train.csv'\n",
    "gdown.download(url, output, quiet=False, verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef46e3",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2adc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "display(data.head()) #  Digits / Pixel data\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d570d",
   "metadata": {},
   "source": [
    "The data set-`train.csv` has 785 columns. The first column, called `label`, which is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "There are 10 labels (0 to 9).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0be4db",
   "metadata": {},
   "source": [
    "# Define a Class PCA to get the Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b193ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    #lbl_col_index=0 # global variable ; column index/position having class labels.\n",
    "    def __init__(self, df, lbl_col_index=0):\n",
    "    \n",
    "        '''\n",
    "        Declare instance variables\n",
    "        '''\n",
    "        self.df=df  \n",
    "        self.lbl_col_ind=lbl_col_index\n",
    "        self.labels = pd.Series(self.df.iloc[:,self.lbl_col_ind]).unique() #unique target labels \n",
    "        self.labelseries = pd.Series(self.df.iloc[:,self.lbl_col_ind])\n",
    "#         print(self.labels)\n",
    "        \n",
    "    def drop_target_column(self, dataframe):\n",
    "        features= dataframe.drop(dataframe.iloc[:,self.lbl_col_ind:self.lbl_col_ind+1], axis=1)\n",
    "        return features\n",
    "            \n",
    "        \n",
    "        \n",
    "    def subset_data(self, uniq_trgt_lbl:int = None):\n",
    "        '''\n",
    "        Takes an integer value as an input (numeric category label).\n",
    "        Returns-\n",
    "        (1) Pandas Dataframe which is scaled subset of data without labels \n",
    "        (2) Pandas Series of Labels \n",
    "        Standard Scaler returns values with zero mean and unit variance.\n",
    "        '''\n",
    "        if uniq_trgt_lbl is None:\n",
    "            subdf = self.df\n",
    "        else:\n",
    "            subdf=self.df[self.df.iloc[:,self.lbl_col_ind] == self.labels[uniq_trgt_lbl]]\n",
    "            \n",
    "        catg = subdf.iloc[:,self.lbl_col_ind]\n",
    "        features = self.drop_target_column(subdf)\n",
    "        return features, catg\n",
    "    \n",
    "    \n",
    "    def feature_scaling(self, features):\n",
    "        scaled_features=StandardScaler(copy=True, with_mean=True).fit_transform(features)\n",
    "        return scaled_features\n",
    "        \n",
    "        \n",
    "    def cov_mat(self, uniq_trgt_lbl: int=None):\n",
    "        '''\n",
    "        Takes in the unique target label to filter the data.\n",
    "        Returns the scaled data of dimensions (4132, 784) \n",
    "        and the covariance matrix of scaled data of dimensions (784, 784).\n",
    "        '''\n",
    "        if uniq_trgt_lbl is None:\n",
    "            subdf=self.drop_target_column(self.df)\n",
    "            scaled_feat=self.feature_scaling(subdf)\n",
    "            covmat = np.cov(scaled_feat, rowvar = False, bias = False)\n",
    "            return scaled_feat, covmat\n",
    "        else:\n",
    "            subdf=self.subset_data(uniq_trgt_lbl)[0] #get the scaled features \n",
    "            scaled_feat=self.feature_scaling(subdf)\n",
    "            covmat = np.cov(scaled_feat, rowvar = False, bias = False)\n",
    "            return scaled_feat, covmat\n",
    "         \n",
    "\n",
    "    def eig_val_eig_vec(self, covariance_matrix):\n",
    "        \n",
    "        '''\n",
    "        Takes input as square array / covariance matrix\n",
    "        and returns pairs of eigen value and eigen vector of the\n",
    "        covariance matrix in descending value of eigen value.\n",
    "        '''\n",
    "        \n",
    "        eigval, eigvec = np.linalg.eig(covariance_matrix)\n",
    "        pairs_eigval_eigvec = [(np.abs(eigval[k]), eigvec[:,k]) for k in range(len(eigval))]\n",
    "        sorted_eg_ev_pairs = sorted(pairs_eigval_eigvec, key=lambda rw: rw[0], reverse=True)  \n",
    "        return sorted_eg_ev_pairs \n",
    "    \n",
    "    def visualize_explained_variance(self, covariance_matrix):\n",
    "        information=self.eig_val_eig_vec(covariance_matrix)\n",
    "        eigval= [i[0] for i in information] #array containing eigen values sorted in descending order\n",
    "        var_exp = [(i/sum(eigval)) for i in eigval] \n",
    "        cum_sum_exp = np.cumsum(var_exp) #cummulative explained variance \n",
    "    \n",
    "        plt.step(range(0,len(cum_sum_exp)),\n",
    "                 cum_sum_exp,\n",
    "                 where='mid',\n",
    "                 label='Cumulative explained variance')\n",
    "        \n",
    "        plt.ylabel('Explained variance ratio')\n",
    "        plt.xlabel('Principal component index')\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def extract_n_principal_components(self, covariance_matrix, reqno_of_pcs):\n",
    "        \n",
    "        '''\n",
    "        Horizontally stacks the top two eigen vectors ordered based on\n",
    "        descending eigen values\n",
    "        '''\n",
    "            \n",
    "        srtd_eg_ev_pair = self.eig_val_eig_vec(covariance_matrix)\n",
    "        stacked_cmpnts=[np.hstack((k[1].reshape(784,1))) for k in srtd_eg_ev_pair[0:reqno_of_pcs]]\n",
    "        stacked_arr=np.asarray(stacked_cmpnts).T\n",
    "        return stacked_arr\n",
    "    \n",
    "    def get_projected_data(self, scaleddata, covariance_matrix, reqno_of_pcs):\n",
    "        '''\n",
    "        Takes the covariance matrix of dimensions D*D \n",
    "        Computes the Dot Product of scaled data for cat0 -> (4132*784)\n",
    "        with the eigen vectors of Covariance Matrix  with highest eigen values (784*2)\n",
    "        Return the projected data set of dimensions (m*2) for example for cat0->(4132*2)\n",
    "        '''\n",
    "        stcked_cmpnts = self.extract_n_principal_components(covariance_matrix, reqno_of_pcs)\n",
    "        projecteddata=pd.DataFrame(scaleddata.dot(stcked_cmpnts))\n",
    "        projecteddata.columns = [\"PC_\" + str(col) for col in projecteddata.columns]\n",
    "        return projecteddata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13263a1",
   "metadata": {},
   "source": [
    "# Extract the Top-30 Principal Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ea4673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC_0</th>\n",
       "      <th>PC_1</th>\n",
       "      <th>PC_2</th>\n",
       "      <th>PC_3</th>\n",
       "      <th>PC_4</th>\n",
       "      <th>PC_5</th>\n",
       "      <th>PC_6</th>\n",
       "      <th>PC_7</th>\n",
       "      <th>PC_8</th>\n",
       "      <th>PC_9</th>\n",
       "      <th>...</th>\n",
       "      <th>PC_21</th>\n",
       "      <th>PC_22</th>\n",
       "      <th>PC_23</th>\n",
       "      <th>PC_24</th>\n",
       "      <th>PC_25</th>\n",
       "      <th>PC_26</th>\n",
       "      <th>PC_27</th>\n",
       "      <th>PC_28</th>\n",
       "      <th>PC_29</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.140478</td>\n",
       "      <td>-5.226445</td>\n",
       "      <td>3.887001</td>\n",
       "      <td>-0.901512</td>\n",
       "      <td>-4.929111</td>\n",
       "      <td>-2.035413</td>\n",
       "      <td>-4.706946</td>\n",
       "      <td>4.767184</td>\n",
       "      <td>-0.230958</td>\n",
       "      <td>-1.460962</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282251</td>\n",
       "      <td>1.390834</td>\n",
       "      <td>1.033241</td>\n",
       "      <td>-2.450662</td>\n",
       "      <td>-0.025323</td>\n",
       "      <td>-0.543821</td>\n",
       "      <td>-2.181736</td>\n",
       "      <td>1.114760</td>\n",
       "      <td>-0.497239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.292332</td>\n",
       "      <td>6.032996</td>\n",
       "      <td>1.308148</td>\n",
       "      <td>-2.383294</td>\n",
       "      <td>-3.095188</td>\n",
       "      <td>1.791095</td>\n",
       "      <td>3.772790</td>\n",
       "      <td>-0.153865</td>\n",
       "      <td>4.115192</td>\n",
       "      <td>-4.299357</td>\n",
       "      <td>...</td>\n",
       "      <td>3.397149</td>\n",
       "      <td>0.212285</td>\n",
       "      <td>-0.220914</td>\n",
       "      <td>0.623992</td>\n",
       "      <td>-4.191473</td>\n",
       "      <td>-0.001298</td>\n",
       "      <td>0.070117</td>\n",
       "      <td>-0.246701</td>\n",
       "      <td>-1.783818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.644503</td>\n",
       "      <td>-1.705813</td>\n",
       "      <td>2.289326</td>\n",
       "      <td>2.241135</td>\n",
       "      <td>-5.094426</td>\n",
       "      <td>4.152058</td>\n",
       "      <td>1.012004</td>\n",
       "      <td>-1.732559</td>\n",
       "      <td>-0.436261</td>\n",
       "      <td>-0.073687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925756</td>\n",
       "      <td>-2.307804</td>\n",
       "      <td>-1.882139</td>\n",
       "      <td>2.069934</td>\n",
       "      <td>-0.594148</td>\n",
       "      <td>-1.129816</td>\n",
       "      <td>-1.538711</td>\n",
       "      <td>0.583776</td>\n",
       "      <td>0.248477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.474207</td>\n",
       "      <td>5.836139</td>\n",
       "      <td>2.008617</td>\n",
       "      <td>4.271106</td>\n",
       "      <td>-2.377777</td>\n",
       "      <td>-2.179913</td>\n",
       "      <td>-4.398030</td>\n",
       "      <td>0.353712</td>\n",
       "      <td>-0.992308</td>\n",
       "      <td>5.501253</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.099471</td>\n",
       "      <td>-0.531728</td>\n",
       "      <td>-0.489333</td>\n",
       "      <td>-3.446940</td>\n",
       "      <td>-1.623493</td>\n",
       "      <td>-0.844744</td>\n",
       "      <td>0.505766</td>\n",
       "      <td>0.655911</td>\n",
       "      <td>0.779984</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.559574</td>\n",
       "      <td>6.024818</td>\n",
       "      <td>0.933179</td>\n",
       "      <td>-3.012645</td>\n",
       "      <td>-9.489179</td>\n",
       "      <td>2.331195</td>\n",
       "      <td>6.149597</td>\n",
       "      <td>1.783637</td>\n",
       "      <td>4.123302</td>\n",
       "      <td>-5.757361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.842493</td>\n",
       "      <td>0.868689</td>\n",
       "      <td>-0.167608</td>\n",
       "      <td>2.809954</td>\n",
       "      <td>-3.381034</td>\n",
       "      <td>-1.239311</td>\n",
       "      <td>0.041742</td>\n",
       "      <td>-1.488019</td>\n",
       "      <td>-0.685522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>13.678849</td>\n",
       "      <td>-1.350366</td>\n",
       "      <td>-3.957336</td>\n",
       "      <td>-5.379672</td>\n",
       "      <td>-10.875898</td>\n",
       "      <td>5.105523</td>\n",
       "      <td>-0.071920</td>\n",
       "      <td>5.084014</td>\n",
       "      <td>4.253677</td>\n",
       "      <td>-0.673734</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.519416</td>\n",
       "      <td>1.800787</td>\n",
       "      <td>2.581044</td>\n",
       "      <td>-0.835002</td>\n",
       "      <td>0.791062</td>\n",
       "      <td>1.092196</td>\n",
       "      <td>0.969723</td>\n",
       "      <td>-4.285741</td>\n",
       "      <td>-1.025209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>-8.869582</td>\n",
       "      <td>-1.187360</td>\n",
       "      <td>2.323167</td>\n",
       "      <td>1.528830</td>\n",
       "      <td>-5.798988</td>\n",
       "      <td>2.821950</td>\n",
       "      <td>0.351780</td>\n",
       "      <td>-0.529810</td>\n",
       "      <td>-0.992204</td>\n",
       "      <td>-1.126098</td>\n",
       "      <td>...</td>\n",
       "      <td>2.021420</td>\n",
       "      <td>-2.783751</td>\n",
       "      <td>-1.306372</td>\n",
       "      <td>1.845894</td>\n",
       "      <td>-0.297738</td>\n",
       "      <td>-0.217034</td>\n",
       "      <td>-0.036322</td>\n",
       "      <td>0.339123</td>\n",
       "      <td>1.303649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>0.495391</td>\n",
       "      <td>7.076277</td>\n",
       "      <td>-12.089700</td>\n",
       "      <td>-3.223278</td>\n",
       "      <td>-0.618203</td>\n",
       "      <td>-0.330449</td>\n",
       "      <td>2.128035</td>\n",
       "      <td>-10.535164</td>\n",
       "      <td>2.225962</td>\n",
       "      <td>-1.881028</td>\n",
       "      <td>...</td>\n",
       "      <td>2.868081</td>\n",
       "      <td>-0.034297</td>\n",
       "      <td>-2.166629</td>\n",
       "      <td>-3.650243</td>\n",
       "      <td>-3.068946</td>\n",
       "      <td>2.451997</td>\n",
       "      <td>-0.601931</td>\n",
       "      <td>-0.825128</td>\n",
       "      <td>-2.011762</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>2.307240</td>\n",
       "      <td>-4.344513</td>\n",
       "      <td>0.699848</td>\n",
       "      <td>10.011222</td>\n",
       "      <td>5.586478</td>\n",
       "      <td>5.494875</td>\n",
       "      <td>-0.189789</td>\n",
       "      <td>-5.450360</td>\n",
       "      <td>-2.181693</td>\n",
       "      <td>-1.767516</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.260995</td>\n",
       "      <td>2.777467</td>\n",
       "      <td>-0.797147</td>\n",
       "      <td>0.717561</td>\n",
       "      <td>-2.477200</td>\n",
       "      <td>1.701241</td>\n",
       "      <td>2.232109</td>\n",
       "      <td>-0.524232</td>\n",
       "      <td>2.044783</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>-4.807670</td>\n",
       "      <td>1.559121</td>\n",
       "      <td>-2.497936</td>\n",
       "      <td>2.218724</td>\n",
       "      <td>1.041887</td>\n",
       "      <td>-0.168182</td>\n",
       "      <td>-1.191325</td>\n",
       "      <td>3.285766</td>\n",
       "      <td>1.626590</td>\n",
       "      <td>-1.431324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363469</td>\n",
       "      <td>1.032882</td>\n",
       "      <td>1.583452</td>\n",
       "      <td>-0.378018</td>\n",
       "      <td>-0.631496</td>\n",
       "      <td>-2.531351</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>-0.871751</td>\n",
       "      <td>-0.898639</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PC_0      PC_1       PC_2       PC_3       PC_4      PC_5  \\\n",
       "0      -5.140478 -5.226445   3.887001  -0.901512  -4.929111 -2.035413   \n",
       "1      19.292332  6.032996   1.308148  -2.383294  -3.095188  1.791095   \n",
       "2      -7.644503 -1.705813   2.289326   2.241135  -5.094426  4.152058   \n",
       "3      -0.474207  5.836139   2.008617   4.271106  -2.377777 -2.179913   \n",
       "4      26.559574  6.024818   0.933179  -3.012645  -9.489179  2.331195   \n",
       "...          ...       ...        ...        ...        ...       ...   \n",
       "41995  13.678849 -1.350366  -3.957336  -5.379672 -10.875898  5.105523   \n",
       "41996  -8.869582 -1.187360   2.323167   1.528830  -5.798988  2.821950   \n",
       "41997   0.495391  7.076277 -12.089700  -3.223278  -0.618203 -0.330449   \n",
       "41998   2.307240 -4.344513   0.699848  10.011222   5.586478  5.494875   \n",
       "41999  -4.807670  1.559121  -2.497936   2.218724   1.041887 -0.168182   \n",
       "\n",
       "           PC_6       PC_7      PC_8      PC_9  ...     PC_21     PC_22  \\\n",
       "0     -4.706946   4.767184 -0.230958 -1.460962  ...  1.282251  1.390834   \n",
       "1      3.772790  -0.153865  4.115192 -4.299357  ...  3.397149  0.212285   \n",
       "2      1.012004  -1.732559 -0.436261 -0.073687  ...  0.925756 -2.307804   \n",
       "3     -4.398030   0.353712 -0.992308  5.501253  ... -1.099471 -0.531728   \n",
       "4      6.149597   1.783637  4.123302 -5.757361  ... -0.842493  0.868689   \n",
       "...         ...        ...       ...       ...  ...       ...       ...   \n",
       "41995 -0.071920   5.084014  4.253677 -0.673734  ... -1.519416  1.800787   \n",
       "41996  0.351780  -0.529810 -0.992204 -1.126098  ...  2.021420 -2.783751   \n",
       "41997  2.128035 -10.535164  2.225962 -1.881028  ...  2.868081 -0.034297   \n",
       "41998 -0.189789  -5.450360 -2.181693 -1.767516  ... -1.260995  2.777467   \n",
       "41999 -1.191325   3.285766  1.626590 -1.431324  ...  0.363469  1.032882   \n",
       "\n",
       "          PC_23     PC_24     PC_25     PC_26     PC_27     PC_28     PC_29  \\\n",
       "0      1.033241 -2.450662 -0.025323 -0.543821 -2.181736  1.114760 -0.497239   \n",
       "1     -0.220914  0.623992 -4.191473 -0.001298  0.070117 -0.246701 -1.783818   \n",
       "2     -1.882139  2.069934 -0.594148 -1.129816 -1.538711  0.583776  0.248477   \n",
       "3     -0.489333 -3.446940 -1.623493 -0.844744  0.505766  0.655911  0.779984   \n",
       "4     -0.167608  2.809954 -3.381034 -1.239311  0.041742 -1.488019 -0.685522   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41995  2.581044 -0.835002  0.791062  1.092196  0.969723 -4.285741 -1.025209   \n",
       "41996 -1.306372  1.845894 -0.297738 -0.217034 -0.036322  0.339123  1.303649   \n",
       "41997 -2.166629 -3.650243 -3.068946  2.451997 -0.601931 -0.825128 -2.011762   \n",
       "41998 -0.797147  0.717561 -2.477200  1.701241  2.232109 -0.524232  2.044783   \n",
       "41999  1.583452 -0.378018 -0.631496 -2.531351  0.536819 -0.871751 -0.898639   \n",
       "\n",
       "       label  \n",
       "0          1  \n",
       "1          0  \n",
       "2          1  \n",
       "3          4  \n",
       "4          0  \n",
       "...      ...  \n",
       "41995      0  \n",
       "41996      1  \n",
       "41997      7  \n",
       "41998      6  \n",
       "41999      9  \n",
       "\n",
       "[42000 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uniq_categories=sorted(data.label.unique())\n",
    "#create PCA instance (object)\n",
    "pca_instance=PCA(data, 0)  ## 0 is the column index of labels in the data.\n",
    "scaled_dat, covmatr=pca_instance.cov_mat()\n",
    "PCA_30=pca_instance.get_projected_data(scaled_dat, covmatr, 30)\n",
    "PCA_30['label'] = pca_instance.labelseries\n",
    "PCA_30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95d272",
   "metadata": {},
   "source": [
    "<br><br>Clustering is the grouping of similar things. There are various clustering methods, for example-\n",
    "<br>\n",
    "![image](https://raw.githubusercontent.com/niteshjindal170988/unsupervised-learning/main/clustering/.scrap/clus0.JPG)\n",
    "<br>\n",
    "**K means clustering** is a type of Partitional Clustering. Let us say that we have n different data points $x_{1}, x_{2} \\ldots x_{n}$ and K different clusters, such as   $\\mathrm{m}_{1}, \\mathrm{~m}_{2} \\ldots \\mathrm{m}_{\\mathrm{k}}$. For every combination of data point and a cluster, we can compute a distance between them, for example the distance between $x_1$ and $m_1$ or distance between $x_1$ and $m_k$ and so on.\n",
    "Minimizing the distance between nth data point and kth cluster can be written as:\n",
    "$\\left(x_n-m_k\\right)^2$                           \n",
    "<br>\n",
    "However, every data point is not associated with every cluster. Thus, we need to associate the nth data point with the cluster it belongs to, not all the clusters. We will define a latent variable $\\delta_{n,k}$ that tells us whether the nth datapoint is associated with cluster K or not. $\\delta_{n,k}$ either 0 or 1. If it is 1, then the distance between the nth datapoint and kth cluster matters else not. <br><br>\n",
    "Therefore, we write the objective function as:\n",
    "<br><br>\n",
    "$J(m)=\\sum_{n} \\sum_{k} \\delta_{n, k}\\left(\\left\\|x_{n}-m_{k}\\right\\|\\right)^{2}$\n",
    "<br>\n",
    "<br>\n",
    "**Params**\t**Term**\t**Definition**<br><br>\n",
    "K\t= Hyperparameter\t= Number of Clusters <br><br>\n",
    "$m_k$\t= Parameters\t= Cluster Mean Vectors.<br><br>\n",
    "$D\\ast k$\t= Total Number of Parameters.\tEach $m_k$ is a D-dimensional vector and hence its matrix size is $D\\ast k$. Thus, the learned parameters are $D\\ast k$<br><br>\n",
    "$\\delta_{n,k}$\t= Latent Parameter<br><br>\n",
    "$\\delta_{n,k}$ that have value of 1 when there is an association between nth datapoint and a particular cluster. For all other combinations, the value would be 0. Latent parameter are similar to parameters, but we need them so as to write the objective function in a simple way.<br>\n",
    "<br> \n",
    "Hyperparameter tell us the complexity of the modelling system.\n",
    "<br><br>\n",
    "What makes $\\delta_{n,k}$ a latent parameter?\n",
    "<br>\n",
    "We learned that the size of $\\delta_{n,k}$ is  $n\\ast k$ and size of $m_k$ is $D\\ast k$ . If we know $\\delta_{n,k}$ , we can compute $m_k$ because we know that each datapoint is associated with a particular cluster only, and thus we can obtain the all the datapoints belonging to a particular cluster and take their mean to get cluster mean vector or cluster centers. So, the latent parameters are equivalent to parameters ( $m_k$ )\n",
    "<br>\n",
    "We can alternate between the $\\delta_{n,k}$ and  ${\\bar{m}}_k$  ( i.e. $\\delta_{n,k}$ is given and known to us and we compute ${\\bar{m}}_k$ , or ${\\bar{m}}_k$ is given and we compute $\\delta_{n,k}$).  In the following lines, we will consider $\\delta_{n,k}$ as constant and update the ${\\bar{m}}_k$ -\n",
    "<br><br>\n",
    "Steps:<br>\n",
    "\n",
    "-1 Cluster Assignment: Randomly Initialize The Cluster Centers\n",
    "<br>\n",
    "![image](https://raw.githubusercontent.com/niteshjindal170988/unsupervised-learning/main/clustering/.scrap/clus1.JPG)\n",
    "<br>\n",
    "-2 Compute the association of each Datapoint with each Cluster center (Euclidean distance). If distance of a datapoint is nearest to a particular cluster, map the datapoint to that cluster and assign $\\delta_{n,k}$ = 1 <br><br>\n",
    "-3 Recompute the Cluster Centers.\n",
    "<br>\n",
    "![image](https://raw.githubusercontent.com/niteshjindal170988/unsupervised-learning/main/clustering/.scrap/clus2.JPG)\n",
    "<br>\n",
    "We keep altering between the step 2 and step 3 until the convergence is met. \n",
    "<br><br>\n",
    "Let us derive the Cluster Centers:<br><br>\n",
    "$J(m, \\delta)=\\sum_{n=1}^{N} \\sum_{k=1}^{k} \\delta_{n, k}\\left(\\bar{m}_{k}-\\bar{x}_{n}\\right)^{2}$\n",
    "<br><br>Keep  $\\delta_{n,k}$ as constant and update the ${\\bar{m}}_j$. We take the partial derivative w.r.t ${\\bar{m}}_j$ (i.e., jth cluster):<br><br>\n",
    "=> $\\frac{\\partial J}{\\partial m_{j}}=2 \\sum_{n=1}^{N} \\delta_{n,j}\\left(\\bar{m}_{j}-\\bar{x}_{n}\\right)=0$<br><br>\n",
    "=> $\\sum_{n=1}^{N} \\delta_{n,j} \\cdot m_{j}$ = $\\sum_{n=1}^{N} \\delta_{n,j} \\bar{x}_{n}$<br><br>\n",
    "=> $\\mathbf{m}_{k}^{(t+1)} = \\frac{\\sum_{n=1}^{N} \\delta_{n, k}^{(t)} \\mathbf{X}^{n}}{\\sum_{n=1}^{N} \\delta_{n, k}^{(t)}}$\n",
    "\n",
    "<br><br>\n",
    "Output of k means clustering is a set of mean vectors. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d0ed3",
   "metadata": {},
   "source": [
    "# Let's get started ! \n",
    "\n",
    "## K-means Clustering on the PCA_30\n",
    "<br>\n",
    "We initialize the random clusters initially and then **iterate** of the datapoints and number of clusters until the convergence is achieved.Therefore, K Means Clustering is an **Iterative Method** as opposed to Closed Form of Solution Method, and is **Non-Deterministic**  as opposed to Deterministic, because the output depends on the initialization of random clusters in the K-means clustering.  Farthest first point initialization with K means clusterings makes the K-means Clustering somewhat deterministic, but if we do randomly initialize first point or density based sampling gives random or non-deterministic output. \n",
    "<br><br>\n",
    "Minimum number of clusters can be one (i.e. all datapoints in a single cluster) and maximum number of clusters can be equal to the number of datapoints in the data. As we **increase the complexity** i.e., the number of clusters,there would be an **increase in accuracy**. Model **memorizes** When the number of cluster becomes equal to number of datapoints, the accuracy is 100%. <br><br>\n",
    "\n",
    "<br><br>**In the following exercise, we will build 10 clusters and examine the effect of initialization on clusters-** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029dfb4",
   "metadata": {},
   "source": [
    "# Define kmeans() \n",
    "\n",
    "Apply k means clustering on batch of data. Each batch is a subset of data filtered with class label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "59d9168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(K, D: int, N: int, batch, cls_labelname):\n",
    "    '''\n",
    "    K is the number of clusters, D and N are batch dimensions and rows. \n",
    "    Batch is a subset of the data filtered with class label.\n",
    "    '''\n",
    "    \n",
    "    cluster_centers_T = np.zeros(shape=(D, K))  \n",
    "\n",
    "    #generate random vectors\n",
    "    for num in range(0, batch.shape[1]):\n",
    "        random.seed(num)\n",
    "        rv= random.sample(range(int(np.mean(batch) - 4.5*np.std(batch)), int(np.mean(batch) + 5.5*np.std(batch))), K)\n",
    "        cluster_centers_T[num] = rv\n",
    "\n",
    "    m_k = np.transpose(cluster_centers_T) # each of the 10 cluster centers is a 30 - dimensional vector.\n",
    "    m_k # cluster centers shape is (K * D) \n",
    "    m_k_initialization_matrix = np.zeros(m_k.shape) #  shape is (K*D) \n",
    "\n",
    "    #The linalg norm() function returns the norm of the given matrix or vector.\n",
    "    #Matrix norm measures the size of a matrix. \n",
    "    err = np.linalg.norm(m_k- m_k_initialization_matrix) \n",
    "    print(\"\\nError during first farther point initialization is {}\\n\".format(err))\n",
    "    iters_to_converge = 0\n",
    "    while err!=0:    \n",
    "        # dictionary contains information about best cluster corresponding to a datapoint in batch\n",
    "        datapoint_cluster_mapping_dict= dict() \n",
    "        delta_n_k = np.zeros(shape=(N, K)) #shape is (N*K)\n",
    "        for rw in range(len(batch)):\n",
    "              #get the euclidean distance between each batch row and all the k=10 cluster centers. \n",
    "            delta_n_k[rw] = np.linalg.norm(x = batch[rw] - m_k, keepdims = False,  axis=1) # (10,)\n",
    "            datapoint_cluster_mapping_dict[rw] = np.argmin(delta_n_k[rw]) \n",
    "        datapoint_cluster_mapping_dict  \n",
    "\n",
    "        m_k_initialization_matrix = deepcopy(m_k)\n",
    "\n",
    "        for cluster in range(0, K): #seggregate the datapoints within each cluster. \n",
    "            cluster_density = [batch[row] for row in range(len(batch)) if datapoint_cluster_mapping_dict[row] == cluster]\n",
    "            m_k[cluster] = np.nan_to_num(np.mean(np.array(cluster_density), axis=0)) # take mean of datapoints in cluster\n",
    "        err = np.linalg.norm(m_k - m_k_initialization_matrix)\n",
    "        if err <= 0.01:\n",
    "            iters_to_converge += 1\n",
    "            #print(\"\\nError in iteration {} is {}\\n\".format(iters_to_converge, err))\n",
    "            break;\n",
    "        else:\n",
    "            iters_to_converge += 1\n",
    "            #print(\"\\nError in iteration {} is {}\".format(iters_to_converge, err))\n",
    "        \n",
    "    print(\"\\nError in iteration {} is {}\\n\".format(iters_to_converge, err))\n",
    "    if cls_labelname is None:\n",
    "        data_cluster_mapping=datapoint_cluster_mapping_dict\n",
    "        return data_cluster_mapping\n",
    "    \n",
    "    else:\n",
    "        inv_datapoint_cluster_mapping_dict =sorted(\n",
    "          Counter(datapoint_cluster_mapping_dict.values()).items())\n",
    "        \n",
    "        output=pd.DataFrame(inv_datapoint_cluster_mapping_dict, \n",
    "                                columns = (\"cluster_number\", \"datapoints_cnt\")\n",
    "                                    ).assign(iters_to_converge=(iters_to_converge)\n",
    "                                            ).assign(class_label=cls_labelname)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9fa3d",
   "metadata": {},
   "source": [
    "# K Means Clustering on entire Dataset PCA_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "543b9d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the Batch is  (42000, 30)\n",
      "\n",
      "Cluster  5\n",
      "\n",
      "Error during first farther point initialization is 109.69503179269333\n",
      "\n",
      "\n",
      "Error in iteration 49 is 0.0054282108863108\n",
      "\n",
      "Purity of Cluster 5 is 0.366452380952381 \n",
      "\n",
      "\n",
      "Cluster  10\n",
      "\n",
      "Error during first farther point initialization is 164.76650144977893\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.005011769715313935\n",
      "\n",
      "Purity of Cluster 10 is 0.44011904761904763 \n",
      "\n",
      "\n",
      "Cluster  15\n",
      "\n",
      "Error during first farther point initialization is 205.64046294443125\n",
      "\n",
      "\n",
      "Error in iteration 195 is 0.009082811843355429\n",
      "\n",
      "Purity of Cluster 15 is 0.5405714285714286 \n",
      "\n",
      "\n",
      "Cluster  20\n",
      "\n",
      "Error during first farther point initialization is 233.18447632722038\n",
      "\n",
      "\n",
      "Error in iteration 145 is 0.00894668506469954\n",
      "\n",
      "Purity of Cluster 20 is 0.5145238095238095 \n",
      "\n",
      "\n",
      "Cluster  25\n",
      "\n",
      "Error during first farther point initialization is 256.81900241220467\n",
      "\n",
      "\n",
      "Error in iteration 74 is 0.0\n",
      "\n",
      "Purity of Cluster 25 is 0.5703809523809524 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Cluster Number'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kmeansobj=PCA(PCA_30, 30)  # label column position in PCA_30 is 30th\n",
    "unq_labels = kmeansobj.labels # 10 unique labels in the digits data\n",
    "f, c = kmeansobj.subset_data(None) \n",
    "\n",
    "batch=f.values\n",
    "D=batch.shape[1]\n",
    "N=batch.shape[0]\n",
    "print(\"\\nShape of the Batch is \", (N,D))\n",
    "Kluster=[5,10,15,20,25]\n",
    "cluster_quality_purity = []\n",
    "for K in Kluster: #range(15, 16):\n",
    "    print(\"\\nCluster \", K)\n",
    "    outp_d2c_mapping=kmeans(K, D, N, batch, None)\n",
    "    outp_mapping_dict=[(outp_d2c_mapping[obs_ind], obs_ind, c[obs_ind])\n",
    "                       for obs_ind in outp_d2c_mapping]\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "    dat=pd.DataFrame(outp_mapping_dict, columns = [\"cluster\", \"observation\", \"actual_label\"])\n",
    "    cls_cnt = dat.groupby(['cluster', 'actual_label']).count()['observation']\n",
    "    tot_cnts_all_cluster = 0\n",
    "    max_cnts_per_cluster=0\n",
    "    for k in range(K):\n",
    "        #print(np.argmax(cls_cnt[k]))  # get the class with maximum counts in a cluster\n",
    "        tot_cnts_all_cluster = tot_cnts_all_cluster + np.sum(cls_cnt[k])\n",
    "        try:\n",
    "            max_cnts_per_cluster = max_cnts_per_cluster + cls_cnt[k][np.argmax(cls_cnt[k])]\n",
    "        except KeyError:\n",
    "            max_cnts_per_cluster = max_cnts_per_cluster + cls_cnt[k][np.argmax(cls_cnt[k])+1]\n",
    "\n",
    "    purity = (max_cnts_per_cluster/tot_cnts_all_cluster)\n",
    "    cluster_quality_purity.append(purity)\n",
    "    print(\"Purity of Cluster {} is {} \\n\".format(K, purity))\n",
    "    \n",
    "    \n",
    "'''Plot Cluster vs Purity:- '''\n",
    "Kluster=list([5,10,15,20,25])\n",
    "cluster_eval=pd.DataFrame(zip(Kluster, cluster_quality_purity),\n",
    "                         columns =['Cluster Number', 'Cluster Quality(Purity)'])\n",
    "cluster_eval.plot(x='Cluster Number', y='Cluster Quality(Purity)', kind='line')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99507b",
   "metadata": {},
   "source": [
    "<br>we see the increase in purity with the increase in number of cluster centers.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddc5b7",
   "metadata": {},
   "source": [
    "# K Means Clustering on subsets of Dataset PCA_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f0c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K means on Sample with class label 1\n",
      "\n",
      "Error during first farther point initialization is 186.95721435665433\n",
      "\n",
      "\n",
      "Error in iteration 1 is 173.8816968232656\n",
      "\n",
      "\n",
      "Error in iteration 2 is 18.879588078232057\n",
      "\n",
      "\n",
      "Error in iteration 3 is 9.750444488378397\n",
      "\n",
      "\n",
      "Error in iteration 4 is 5.48953508097156\n",
      "\n",
      "\n",
      "Error in iteration 5 is 3.691137671266259\n",
      "\n",
      "\n",
      "Error in iteration 6 is 3.3148633348351417\n",
      "\n",
      "\n",
      "Error in iteration 7 is 2.8999781401001417\n",
      "\n",
      "\n",
      "Error in iteration 8 is 2.4029316034049057\n",
      "\n",
      "\n",
      "Error in iteration 9 is 1.7602241527387585\n",
      "\n",
      "\n",
      "Error in iteration 10 is 1.3553015768483598\n",
      "\n",
      "\n",
      "Error in iteration 11 is 1.1173191327691872\n",
      "\n",
      "\n",
      "Error in iteration 12 is 1.1215159145664158\n",
      "\n",
      "\n",
      "Error in iteration 13 is 1.1297611455422192\n",
      "\n",
      "\n",
      "Error in iteration 14 is 1.1116144381372275\n",
      "\n",
      "\n",
      "Error in iteration 15 is 1.1310223566334894\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.9737026132865807\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.7907872998388763\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.7908996775966198\n",
      "\n",
      "\n",
      "Error in iteration 19 is 1.0104492894737516\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.981841890967396\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.8960062493013097\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.8876712223607222\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.7915941098893359\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.7799131427313849\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.7837232752811656\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.5016384810286396\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.40352561263581915\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.4007265731543433\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.31890319346612994\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.2121953189763249\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.2300622373737074\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.06706294137719242\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.05972576488453603\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.054678709582263146\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.043288206239402854\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 0\n",
      "\n",
      "Error during first farther point initialization is 113.93857994551274\n",
      "\n",
      "\n",
      "Error in iteration 1 is 109.74229414912543\n",
      "\n",
      "\n",
      "Error in iteration 2 is 13.683081991609201\n",
      "\n",
      "\n",
      "Error in iteration 3 is 8.341834280904939\n",
      "\n",
      "\n",
      "Error in iteration 4 is 9.705263492829488\n",
      "\n",
      "\n",
      "Error in iteration 5 is 6.354516256360314\n",
      "\n",
      "\n",
      "Error in iteration 6 is 4.0581031325256\n",
      "\n",
      "\n",
      "Error in iteration 7 is 4.816744489965359\n",
      "\n",
      "\n",
      "Error in iteration 8 is 18.88847014110714\n",
      "\n",
      "\n",
      "Error in iteration 9 is 3.3167889570970956\n",
      "\n",
      "\n",
      "Error in iteration 10 is 2.2977328051512713\n",
      "\n",
      "\n",
      "Error in iteration 11 is 1.2242845120067942\n",
      "\n",
      "\n",
      "Error in iteration 12 is 1.6997634175313152\n",
      "\n",
      "\n",
      "Error in iteration 13 is 0.9042592475286614\n",
      "\n",
      "\n",
      "Error in iteration 14 is 0.4624559578717069\n",
      "\n",
      "\n",
      "Error in iteration 15 is 0.3620537392312291\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.21425327615202447\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.7166308656775887\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.5017941693829666\n",
      "\n",
      "\n",
      "Error in iteration 19 is 0.5653141385824358\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.3392269702271377\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.38128944432989736\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.2857129697517921\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.18265524305173197\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.14153461597491185\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.1458603776389438\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.13656989723916096\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.07549345770651446\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.10332892224026738\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.12650711322669478\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.09680339854820001\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.1239539810308344\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.13887418262146545\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.13408073939578244\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.049551452262844325\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.02392794723481621\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.018116714111170745\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.021708898606227156\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 4\n",
      "\n",
      "Error during first farther point initialization is 165.10905486980417\n",
      "\n",
      "\n",
      "Error in iteration 1 is 155.20426744107243\n",
      "\n",
      "\n",
      "Error in iteration 2 is 14.55079043539101\n",
      "\n",
      "\n",
      "Error in iteration 3 is 13.55750223696492\n",
      "\n",
      "\n",
      "Error in iteration 4 is 5.5564124015315395\n",
      "\n",
      "\n",
      "Error in iteration 5 is 3.4917445088974683\n",
      "\n",
      "\n",
      "Error in iteration 6 is 3.6240199311586427\n",
      "\n",
      "\n",
      "Error in iteration 7 is 3.9306156733474396\n",
      "\n",
      "\n",
      "Error in iteration 8 is 3.8478870479358984\n",
      "\n",
      "\n",
      "Error in iteration 9 is 2.715180524697563\n",
      "\n",
      "\n",
      "Error in iteration 10 is 2.092593986443908\n",
      "\n",
      "\n",
      "Error in iteration 11 is 1.771667062329392\n",
      "\n",
      "\n",
      "Error in iteration 12 is 1.5455717769212005\n",
      "\n",
      "\n",
      "Error in iteration 13 is 1.3264803678036763\n",
      "\n",
      "\n",
      "Error in iteration 14 is 1.340764332376313\n",
      "\n",
      "\n",
      "Error in iteration 15 is 1.227856176812652\n",
      "\n",
      "\n",
      "Error in iteration 16 is 1.479852325426873\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.990700113335272\n",
      "\n",
      "\n",
      "Error in iteration 18 is 1.1934427084658203\n",
      "\n",
      "\n",
      "Error in iteration 19 is 1.0159599924201408\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.7091317748737144\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.5516807165900577\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.5027236166778387\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.6525166095223426\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.41553598803237074\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.3928259171748659\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.23332508988442416\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.07763091745609577\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 7\n",
      "\n",
      "Error during first farther point initialization is 147.91889669680478\n",
      "\n",
      "\n",
      "Error in iteration 1 is 139.6619900982041\n",
      "\n",
      "\n",
      "Error in iteration 2 is 16.623099098405838\n",
      "\n",
      "\n",
      "Error in iteration 3 is 11.090969342293414\n",
      "\n",
      "\n",
      "Error in iteration 4 is 8.708564234537308\n",
      "\n",
      "\n",
      "Error in iteration 5 is 5.211586713223003\n",
      "\n",
      "\n",
      "Error in iteration 6 is 5.400416303128761\n",
      "\n",
      "\n",
      "Error in iteration 7 is 6.243861464018177\n",
      "\n",
      "\n",
      "Error in iteration 8 is 4.098022432249463\n",
      "\n",
      "\n",
      "Error in iteration 9 is 2.251935368595687\n",
      "\n",
      "\n",
      "Error in iteration 10 is 1.1070709675708406\n",
      "\n",
      "\n",
      "Error in iteration 11 is 0.8347290218975131\n",
      "\n",
      "\n",
      "Error in iteration 12 is 0.6256880446730942\n",
      "\n",
      "\n",
      "Error in iteration 13 is 0.4701931782143668\n",
      "\n",
      "\n",
      "Error in iteration 14 is 0.3733223173441623\n",
      "\n",
      "\n",
      "Error in iteration 15 is 0.29617724198061035\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.12193528210648338\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.11194651191337869\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.1353976138030526\n",
      "\n",
      "\n",
      "Error in iteration 19 is 0.3594972007062877\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.16734849863628637\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.08095123407592936\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.18128974850393736\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.14143267813122415\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.10344044249543835\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.25864724570491965\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.39710450714092604\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.19579110312048917\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.23851315822270017\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.42911431911579656\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.3380884688683292\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.39247708150943933\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.32744563664804605\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.28423185580388455\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.27811760193028845\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.30510971665303777\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.3062928834331506\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.2502312541055129\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.16146715189420452\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.10996077589155963\n",
      "\n",
      "\n",
      "Error in iteration 40 is 0.13373461853791418\n",
      "\n",
      "\n",
      "Error in iteration 41 is 0.09881211008843804\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.13307092472815218\n",
      "\n",
      "\n",
      "Error in iteration 43 is 0.13665321833386923\n",
      "\n",
      "\n",
      "Error in iteration 44 is 0.14478751059898334\n",
      "\n",
      "\n",
      "Error in iteration 45 is 0.21781017741675948\n",
      "\n",
      "\n",
      "Error in iteration 46 is 0.14436531695928076\n",
      "\n",
      "\n",
      "Error in iteration 47 is 0.10967572637015373\n",
      "\n",
      "\n",
      "Error in iteration 48 is 0.1602137291124116\n",
      "\n",
      "\n",
      "Error in iteration 49 is 0.2402284664672612\n",
      "\n",
      "\n",
      "Error in iteration 50 is 0.24853918557566124\n",
      "\n",
      "\n",
      "Error in iteration 51 is 0.30437479578200527\n",
      "\n",
      "\n",
      "Error in iteration 52 is 0.3536825162624271\n",
      "\n",
      "\n",
      "Error in iteration 53 is 0.2252262327005978\n",
      "\n",
      "\n",
      "Error in iteration 54 is 0.19961291760820904\n",
      "\n",
      "\n",
      "Error in iteration 55 is 0.2282782317089944\n",
      "\n",
      "\n",
      "Error in iteration 56 is 0.1776027875065856\n",
      "\n",
      "\n",
      "Error in iteration 57 is 0.34183576430717305\n",
      "\n",
      "\n",
      "Error in iteration 58 is 0.23305968855176892\n",
      "\n",
      "\n",
      "Error in iteration 59 is 0.2704086192746264\n",
      "\n",
      "\n",
      "Error in iteration 60 is 0.3595550469867172\n",
      "\n",
      "\n",
      "Error in iteration 61 is 0.295225987086699\n",
      "\n",
      "\n",
      "Error in iteration 62 is 0.23271719109000252\n",
      "\n",
      "\n",
      "Error in iteration 63 is 0.20084069052161424\n",
      "\n",
      "\n",
      "Error in iteration 64 is 0.19340852097145902\n",
      "\n",
      "\n",
      "Error in iteration 65 is 0.17132267371321558\n",
      "\n",
      "\n",
      "Error in iteration 66 is 0.11939849923256746\n",
      "\n",
      "\n",
      "Error in iteration 67 is 0.10578619491594693\n",
      "\n",
      "\n",
      "Error in iteration 68 is 0.15725675975255338\n",
      "\n",
      "\n",
      "Error in iteration 69 is 0.13050871475352882\n",
      "\n",
      "\n",
      "Error in iteration 70 is 0.047809946928669696\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in iteration 71 is 0.05652160392627956\n",
      "\n",
      "\n",
      "Error in iteration 72 is 0.06210483351861136\n",
      "\n",
      "\n",
      "Error in iteration 73 is 0.04196076536471564\n",
      "\n",
      "\n",
      "Error in iteration 74 is 0.08769058242325443\n",
      "\n",
      "\n",
      "Error in iteration 75 is 0.09435031449064028\n",
      "\n",
      "\n",
      "Error in iteration 76 is 0.10811469646881124\n",
      "\n",
      "\n",
      "Error in iteration 77 is 0.11054759779956533\n",
      "\n",
      "\n",
      "Error in iteration 78 is 0.10161630235071274\n",
      "\n",
      "\n",
      "Error in iteration 79 is 0.08146450138931588\n",
      "\n",
      "\n",
      "Error in iteration 80 is 0.0871539936811382\n",
      "\n",
      "\n",
      "Error in iteration 81 is 0.07102405646860149\n",
      "\n",
      "\n",
      "Error in iteration 82 is 0.13971828501668007\n",
      "\n",
      "\n",
      "Error in iteration 83 is 0.1437066822626444\n",
      "\n",
      "\n",
      "Error in iteration 84 is 0.15465992054089855\n",
      "\n",
      "\n",
      "Error in iteration 85 is 0.14034504324093713\n",
      "\n",
      "\n",
      "Error in iteration 86 is 0.20083294873653504\n",
      "\n",
      "\n",
      "Error in iteration 87 is 0.14162995569500822\n",
      "\n",
      "\n",
      "Error in iteration 88 is 0.17428877190451939\n",
      "\n",
      "\n",
      "Error in iteration 89 is 0.06818277200594522\n",
      "\n",
      "\n",
      "Error in iteration 90 is 0.09768242574830074\n",
      "\n",
      "\n",
      "Error in iteration 91 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 3\n",
      "\n",
      "Error during first farther point initialization is 169.4609099468075\n",
      "\n",
      "\n",
      "Error in iteration 1 is 177.14051102407288\n",
      "\n",
      "\n",
      "Error in iteration 2 is 33.51354838543716\n",
      "\n",
      "\n",
      "Error in iteration 3 is 21.250164279359232\n",
      "\n",
      "\n",
      "Error in iteration 4 is 9.868201652229386\n",
      "\n",
      "\n",
      "Error in iteration 5 is 6.471212040909194\n",
      "\n",
      "\n",
      "Error in iteration 6 is 6.02158342581872\n",
      "\n",
      "\n",
      "Error in iteration 7 is 7.4980884465343784\n",
      "\n",
      "\n",
      "Error in iteration 8 is 6.018474822533634\n",
      "\n",
      "\n",
      "Error in iteration 9 is 3.8918416661477515\n",
      "\n",
      "\n",
      "Error in iteration 10 is 4.647707910293379\n",
      "\n",
      "\n",
      "Error in iteration 11 is 2.3500791450064864\n",
      "\n",
      "\n",
      "Error in iteration 12 is 2.811295791486095\n",
      "\n",
      "\n",
      "Error in iteration 13 is 1.9319078696714962\n",
      "\n",
      "\n",
      "Error in iteration 14 is 1.1809718999969625\n",
      "\n",
      "\n",
      "Error in iteration 15 is 1.7428433560252452\n",
      "\n",
      "\n",
      "Error in iteration 16 is 2.1810497374304423\n",
      "\n",
      "\n",
      "Error in iteration 17 is 1.2595587948235585\n",
      "\n",
      "\n",
      "Error in iteration 18 is 2.0418507646475264\n",
      "\n",
      "\n",
      "Error in iteration 19 is 1.26748242154892\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.7119679421960863\n",
      "\n",
      "\n",
      "Error in iteration 21 is 1.6458128741705653\n",
      "\n",
      "\n",
      "Error in iteration 22 is 2.669707466324958\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.8291229982347393\n",
      "\n",
      "\n",
      "Error in iteration 24 is 1.0032701280763383\n",
      "\n",
      "\n",
      "Error in iteration 25 is 1.2874692114857325\n",
      "\n",
      "\n",
      "Error in iteration 26 is 1.3274431414825023\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.8849808931143579\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.7946779902577749\n",
      "\n",
      "\n",
      "Error in iteration 29 is 1.3795701493857402\n",
      "\n",
      "\n",
      "Error in iteration 30 is 1.2231285799951261\n",
      "\n",
      "\n",
      "Error in iteration 31 is 1.3325697139275585\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.91694229619455\n",
      "\n",
      "\n",
      "Error in iteration 33 is 1.0765573944426399\n",
      "\n",
      "\n",
      "Error in iteration 34 is 1.0303531600492744\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.8117973502426645\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.6383517186183408\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.5247190280920128\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.19733347866740103\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.17800375191682904\n",
      "\n",
      "\n",
      "Error in iteration 40 is 0.13994249170024714\n",
      "\n",
      "\n",
      "Error in iteration 41 is 0.1952948207867323\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.06498260277385463\n",
      "\n",
      "\n",
      "Error in iteration 43 is 0.04903792928640799\n",
      "\n",
      "\n",
      "Error in iteration 44 is 0.04617596136553577\n",
      "\n",
      "\n",
      "Error in iteration 45 is 0.06306977808042513\n",
      "\n",
      "\n",
      "Error in iteration 46 is 0.1872685272682648\n",
      "\n",
      "\n",
      "Error in iteration 47 is 0.33042974576621315\n",
      "\n",
      "\n",
      "Error in iteration 48 is 0.30651201169224745\n",
      "\n",
      "\n",
      "Error in iteration 49 is 0.4128814135995831\n",
      "\n",
      "\n",
      "Error in iteration 50 is 0.157870250387926\n",
      "\n",
      "\n",
      "Error in iteration 51 is 0.1487346109113868\n",
      "\n",
      "\n",
      "Error in iteration 52 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 5\n",
      "\n",
      "Error during first farther point initialization is 147.91889669680478\n",
      "\n",
      "\n",
      "Error in iteration 1 is 141.14637907430952\n",
      "\n",
      "\n",
      "Error in iteration 2 is 11.401460085072731\n",
      "\n",
      "\n",
      "Error in iteration 3 is 7.99962828415533\n",
      "\n",
      "\n",
      "Error in iteration 4 is 6.910347307151712\n",
      "\n",
      "\n",
      "Error in iteration 5 is 5.3847147005571125\n",
      "\n",
      "\n",
      "Error in iteration 6 is 3.933676983608443\n",
      "\n",
      "\n",
      "Error in iteration 7 is 2.8712156724507354\n",
      "\n",
      "\n",
      "Error in iteration 8 is 1.981185245122173\n",
      "\n",
      "\n",
      "Error in iteration 9 is 1.587754950402083\n",
      "\n",
      "\n",
      "Error in iteration 10 is 1.271051070636703\n",
      "\n",
      "\n",
      "Error in iteration 11 is 1.1166339670259025\n",
      "\n",
      "\n",
      "Error in iteration 12 is 0.9566981741315883\n",
      "\n",
      "\n",
      "Error in iteration 13 is 0.6593586330984857\n",
      "\n",
      "\n",
      "Error in iteration 14 is 0.5258753952245768\n",
      "\n",
      "\n",
      "Error in iteration 15 is 0.4659506128101307\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.3263227107888978\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.5327426041591913\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.48152135945741265\n",
      "\n",
      "\n",
      "Error in iteration 19 is 0.5502575859972638\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.5478203539081025\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.5426265137100499\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.5835519154860381\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.6164340211131348\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.4925852783885014\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.5910506768154206\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.6019801566031726\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.3515954632664444\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.3827304749242749\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.35484066724486496\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.2184297513511468\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.26519855513863866\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.27765021529596645\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.3504420941392347\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.3909080457806421\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.47168608137769685\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.445891813488307\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.3727438093141005\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.32293807774607936\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.23320055553528038\n",
      "\n",
      "\n",
      "Error in iteration 40 is 0.12127279190865317\n",
      "\n",
      "\n",
      "Error in iteration 41 is 0.2230216167937641\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.2649596626171517\n",
      "\n",
      "\n",
      "Error in iteration 43 is 0.2636171232366787\n",
      "\n",
      "\n",
      "Error in iteration 44 is 0.3709582034006432\n",
      "\n",
      "\n",
      "Error in iteration 45 is 0.3511233512285174\n",
      "\n",
      "\n",
      "Error in iteration 46 is 0.2620937389344335\n",
      "\n",
      "\n",
      "Error in iteration 47 is 0.2776420415943985\n",
      "\n",
      "\n",
      "Error in iteration 48 is 0.2579456074561479\n",
      "\n",
      "\n",
      "Error in iteration 49 is 0.27980869052029245\n",
      "\n",
      "\n",
      "Error in iteration 50 is 0.2555576278667293\n",
      "\n",
      "\n",
      "Error in iteration 51 is 0.1799416854805503\n",
      "\n",
      "\n",
      "Error in iteration 52 is 0.24087094482073654\n",
      "\n",
      "\n",
      "Error in iteration 53 is 0.18563983871490702\n",
      "\n",
      "\n",
      "Error in iteration 54 is 0.19821309680726312\n",
      "\n",
      "\n",
      "Error in iteration 55 is 0.23058800640458244\n",
      "\n",
      "\n",
      "Error in iteration 56 is 0.20449356172168426\n",
      "\n",
      "\n",
      "Error in iteration 57 is 0.15727339040948018\n",
      "\n",
      "\n",
      "Error in iteration 58 is 0.15021657766575694\n",
      "\n",
      "\n",
      "Error in iteration 59 is 0.0993764478304695\n",
      "\n",
      "\n",
      "Error in iteration 60 is 0.04066070554273513\n",
      "\n",
      "\n",
      "Error in iteration 61 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 8\n",
      "\n",
      "Error during first farther point initialization is 182.71562604221896\n",
      "\n",
      "\n",
      "Error in iteration 1 is 172.65274376134374\n",
      "\n",
      "\n",
      "Error in iteration 2 is 30.08827264123508\n",
      "\n",
      "\n",
      "Error in iteration 3 is 13.840856330027275\n",
      "\n",
      "\n",
      "Error in iteration 4 is 8.901429942482011\n",
      "\n",
      "\n",
      "Error in iteration 5 is 6.272899800010898\n",
      "\n",
      "\n",
      "Error in iteration 6 is 4.06623907832569\n",
      "\n",
      "\n",
      "Error in iteration 7 is 2.694691713361461\n",
      "\n",
      "\n",
      "Error in iteration 8 is 1.6590740004351954\n",
      "\n",
      "\n",
      "Error in iteration 9 is 1.136721593067531\n",
      "\n",
      "\n",
      "Error in iteration 10 is 0.8565432696311486\n",
      "\n",
      "\n",
      "Error in iteration 11 is 0.8999538787798036\n",
      "\n",
      "\n",
      "Error in iteration 12 is 0.9045794120335895\n",
      "\n",
      "\n",
      "Error in iteration 13 is 0.8002848646955502\n",
      "\n",
      "\n",
      "Error in iteration 14 is 0.6225336464609325\n",
      "\n",
      "\n",
      "Error in iteration 15 is 0.7487338013316064\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.4877470802400136\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.4676628868021752\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.49259651944117216\n",
      "\n",
      "\n",
      "Error in iteration 19 is 0.5223044361774523\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.45103259610109864\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.3089144444828816\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.5099302040794874\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.46206009701911144\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.2537533410609628\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.26971401085455887\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.17866803885158164\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.1813004532845493\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.19641083900135084\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.23034365817511276\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.1799054107944175\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.18661647516394111\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.11208579168626881\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.16028469888375213\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.15605070767870985\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.050482288467639067\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.10523967059915994\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.11561250271308973\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.16991090907862005\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.21282681549517668\n",
      "\n",
      "\n",
      "Error in iteration 40 is 0.26543657101185975\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in iteration 41 is 0.33872453588996915\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.37649675559396534\n",
      "\n",
      "\n",
      "Error in iteration 43 is 0.3714872200519606\n",
      "\n",
      "\n",
      "Error in iteration 44 is 0.29853980939670505\n",
      "\n",
      "\n",
      "Error in iteration 45 is 0.2210007164995618\n",
      "\n",
      "\n",
      "Error in iteration 46 is 0.25112278145258593\n",
      "\n",
      "\n",
      "Error in iteration 47 is 0.2836576468932699\n",
      "\n",
      "\n",
      "Error in iteration 48 is 0.2495603028495625\n",
      "\n",
      "\n",
      "Error in iteration 49 is 0.23736963061225283\n",
      "\n",
      "\n",
      "Error in iteration 50 is 0.1667755137847913\n",
      "\n",
      "\n",
      "Error in iteration 51 is 0.2271614634753751\n",
      "\n",
      "\n",
      "Error in iteration 52 is 0.04634368209788699\n",
      "\n",
      "\n",
      "Error in iteration 53 is 0.0656890763674779\n",
      "\n",
      "\n",
      "Error in iteration 54 is 0.09319739731911629\n",
      "\n",
      "\n",
      "Error in iteration 55 is 0.026857451843471585\n",
      "\n",
      "\n",
      "Error in iteration 56 is 0.03093154271706225\n",
      "\n",
      "\n",
      "Error in iteration 57 is 0.026153793888803956\n",
      "\n",
      "\n",
      "Error in iteration 58 is 0.03802349829165362\n",
      "\n",
      "\n",
      "Error in iteration 59 is 0.0287368323260648\n",
      "\n",
      "\n",
      "Error in iteration 60 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 9\n",
      "\n",
      "Error during first farther point initialization is 168.28250057566888\n",
      "\n",
      "\n",
      "Error in iteration 1 is 160.77920212606034\n",
      "\n",
      "\n",
      "Error in iteration 2 is 17.748182227980962\n",
      "\n",
      "\n",
      "Error in iteration 3 is 8.340826474041858\n",
      "\n",
      "\n",
      "Error in iteration 4 is 4.95226078817286\n",
      "\n",
      "\n",
      "Error in iteration 5 is 4.297971368550817\n",
      "\n",
      "\n",
      "Error in iteration 6 is 2.8900037985097153\n",
      "\n",
      "\n",
      "Error in iteration 7 is 2.2608098463491366\n",
      "\n",
      "\n",
      "Error in iteration 8 is 1.9322226730989915\n",
      "\n",
      "\n",
      "Error in iteration 9 is 1.8443575726952983\n",
      "\n",
      "\n",
      "Error in iteration 10 is 1.8814739018208204\n",
      "\n",
      "\n",
      "Error in iteration 11 is 1.7472509573577728\n",
      "\n",
      "\n",
      "Error in iteration 12 is 1.6249523651075222\n",
      "\n",
      "\n",
      "Error in iteration 13 is 1.5062993699959526\n",
      "\n",
      "\n",
      "Error in iteration 14 is 1.2392217618876495\n",
      "\n",
      "\n",
      "Error in iteration 15 is 1.0849517213407438\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.945502656126466\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.843869668035846\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.6708295604012195\n",
      "\n",
      "\n",
      "Error in iteration 19 is 0.5453776345689909\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.812346900432705\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.8076321995932482\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.34064384741446946\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.3326710162268812\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.926011623039296\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.4445194541780112\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.4887599345172771\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.4640990598575729\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.3402676981938595\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.3549216840033817\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.32655643582010807\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.2463903073336214\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.29976530778395716\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.37623218573875783\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.47159538580319416\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.4902571075706996\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.45105699785192294\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.32961256660113075\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.351645622637012\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.4287636578278309\n",
      "\n",
      "\n",
      "Error in iteration 40 is 0.5245816734451513\n",
      "\n",
      "\n",
      "Error in iteration 41 is 0.4749045220011412\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.3291634282750399\n",
      "\n",
      "\n",
      "Error in iteration 43 is 0.3733427295202662\n",
      "\n",
      "\n",
      "Error in iteration 44 is 0.20553487334450665\n",
      "\n",
      "\n",
      "Error in iteration 45 is 0.2226194501616349\n",
      "\n",
      "\n",
      "Error in iteration 46 is 0.3374258545600957\n",
      "\n",
      "\n",
      "Error in iteration 47 is 0.24733018007669474\n",
      "\n",
      "\n",
      "Error in iteration 48 is 0.21553218081478387\n",
      "\n",
      "\n",
      "Error in iteration 49 is 0.7296583814026262\n",
      "\n",
      "\n",
      "Error in iteration 50 is 0.35234659706636084\n",
      "\n",
      "\n",
      "Error in iteration 51 is 0.3519038890976072\n",
      "\n",
      "\n",
      "Error in iteration 52 is 0.34010075718915084\n",
      "\n",
      "\n",
      "Error in iteration 53 is 0.34061778141924326\n",
      "\n",
      "\n",
      "Error in iteration 54 is 0.27605717063731017\n",
      "\n",
      "\n",
      "Error in iteration 55 is 0.1738142602490579\n",
      "\n",
      "\n",
      "Error in iteration 56 is 0.13686937966928706\n",
      "\n",
      "\n",
      "Error in iteration 57 is 0.12503960632741498\n",
      "\n",
      "\n",
      "Error in iteration 58 is 0.187964599643044\n",
      "\n",
      "\n",
      "Error in iteration 59 is 0.3133512654940288\n",
      "\n",
      "\n",
      "Error in iteration 60 is 0.3310951705046141\n",
      "\n",
      "\n",
      "Error in iteration 61 is 0.303450190022068\n",
      "\n",
      "\n",
      "Error in iteration 62 is 0.3511068397775771\n",
      "\n",
      "\n",
      "Error in iteration 63 is 0.292880536657058\n",
      "\n",
      "\n",
      "Error in iteration 64 is 0.3038321040704651\n",
      "\n",
      "\n",
      "Error in iteration 65 is 0.33136892686495706\n",
      "\n",
      "\n",
      "Error in iteration 66 is 0.358302270094055\n",
      "\n",
      "\n",
      "Error in iteration 67 is 0.4825502430619431\n",
      "\n",
      "\n",
      "Error in iteration 68 is 0.37125826878989604\n",
      "\n",
      "\n",
      "Error in iteration 69 is 0.40367928004141557\n",
      "\n",
      "\n",
      "Error in iteration 70 is 0.4226559250376552\n",
      "\n",
      "\n",
      "Error in iteration 71 is 0.471545225720162\n",
      "\n",
      "\n",
      "Error in iteration 72 is 0.4901906337598265\n",
      "\n",
      "\n",
      "Error in iteration 73 is 1.1779439635990396\n",
      "\n",
      "\n",
      "Error in iteration 74 is 0.43568075903361625\n",
      "\n",
      "\n",
      "Error in iteration 75 is 0.5755990579217424\n",
      "\n",
      "\n",
      "Error in iteration 76 is 0.477236788943259\n",
      "\n",
      "\n",
      "Error in iteration 77 is 0.47960428911916536\n",
      "\n",
      "\n",
      "Error in iteration 78 is 0.5251196166988792\n",
      "\n",
      "\n",
      "Error in iteration 79 is 0.5797179203565256\n",
      "\n",
      "\n",
      "Error in iteration 80 is 0.48332553081980895\n",
      "\n",
      "\n",
      "Error in iteration 81 is 0.3975285701759877\n",
      "\n",
      "\n",
      "Error in iteration 82 is 0.4194613574349067\n",
      "\n",
      "\n",
      "Error in iteration 83 is 0.30503874332357717\n",
      "\n",
      "\n",
      "Error in iteration 84 is 0.40748052832574694\n",
      "\n",
      "\n",
      "Error in iteration 85 is 0.3877361246196448\n",
      "\n",
      "\n",
      "Error in iteration 86 is 0.3370641860344671\n",
      "\n",
      "\n",
      "Error in iteration 87 is 0.30573349504900005\n",
      "\n",
      "\n",
      "Error in iteration 88 is 0.3280090025369904\n",
      "\n",
      "\n",
      "Error in iteration 89 is 0.3083993902170497\n",
      "\n",
      "\n",
      "Error in iteration 90 is 0.30783835497224504\n",
      "\n",
      "\n",
      "Error in iteration 91 is 0.49439462246242594\n",
      "\n",
      "\n",
      "Error in iteration 92 is 0.5297571298188742\n",
      "\n",
      "\n",
      "Error in iteration 93 is 0.28376551375420456\n",
      "\n",
      "\n",
      "Error in iteration 94 is 0.30883820295736913\n",
      "\n",
      "\n",
      "Error in iteration 95 is 0.48052997158121175\n",
      "\n",
      "\n",
      "Error in iteration 96 is 0.4013515046836531\n",
      "\n",
      "\n",
      "Error in iteration 97 is 0.5380028918143309\n",
      "\n",
      "\n",
      "Error in iteration 98 is 0.3991058068172031\n",
      "\n",
      "\n",
      "Error in iteration 99 is 0.5898793963287345\n",
      "\n",
      "\n",
      "Error in iteration 100 is 0.7890783022225445\n",
      "\n",
      "\n",
      "Error in iteration 101 is 0.25767858162276563\n",
      "\n",
      "\n",
      "Error in iteration 102 is 0.27060664978777005\n",
      "\n",
      "\n",
      "Error in iteration 103 is 0.38218295003498687\n",
      "\n",
      "\n",
      "Error in iteration 104 is 0.2183533162154878\n",
      "\n",
      "\n",
      "Error in iteration 105 is 0.23288897360062288\n",
      "\n",
      "\n",
      "Error in iteration 106 is 0.3108566683966868\n",
      "\n",
      "\n",
      "Error in iteration 107 is 0.4689534514476679\n",
      "\n",
      "\n",
      "Error in iteration 108 is 0.4422380272825882\n",
      "\n",
      "\n",
      "Error in iteration 109 is 0.19193215801428926\n",
      "\n",
      "\n",
      "Error in iteration 110 is 0.33484500979488346\n",
      "\n",
      "\n",
      "Error in iteration 111 is 0.13057055709308685\n",
      "\n",
      "\n",
      "Error in iteration 112 is 0.17310063385890284\n",
      "\n",
      "\n",
      "Error in iteration 113 is 0.05226580036618537\n",
      "\n",
      "\n",
      "Error in iteration 114 is 0.1212163681009904\n",
      "\n",
      "\n",
      "Error in iteration 115 is 0.20606416679384246\n",
      "\n",
      "\n",
      "Error in iteration 116 is 0.12478427887594938\n",
      "\n",
      "\n",
      "Error in iteration 117 is 0.060050347219827424\n",
      "\n",
      "\n",
      "Error in iteration 118 is 0.07297568737785859\n",
      "\n",
      "\n",
      "Error in iteration 119 is 0.027894687014966665\n",
      "\n",
      "\n",
      "Error in iteration 120 is 0.02438952948616043\n",
      "\n",
      "\n",
      "Error in iteration 121 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 2\n",
      "\n",
      "Error during first farther point initialization is 138.44854639901425\n",
      "\n",
      "\n",
      "Error in iteration 1 is 131.1001912850315\n",
      "\n",
      "\n",
      "Error in iteration 2 is 31.192596501259615\n",
      "\n",
      "\n",
      "Error in iteration 3 is 14.823757533908356\n",
      "\n",
      "\n",
      "Error in iteration 4 is 7.499391237948256\n",
      "\n",
      "\n",
      "Error in iteration 5 is 4.975916896906579\n",
      "\n",
      "\n",
      "Error in iteration 6 is 3.5204635048160022\n",
      "\n",
      "\n",
      "Error in iteration 7 is 3.911624788334389\n",
      "\n",
      "\n",
      "Error in iteration 8 is 2.850593384138485\n",
      "\n",
      "\n",
      "Error in iteration 9 is 2.437100193324255\n",
      "\n",
      "\n",
      "Error in iteration 10 is 1.674218170668242\n",
      "\n",
      "\n",
      "Error in iteration 11 is 1.2262222446814486\n",
      "\n",
      "\n",
      "Error in iteration 12 is 0.4487981028357597\n",
      "\n",
      "\n",
      "Error in iteration 13 is 0.2525900869189965\n",
      "\n",
      "\n",
      "Error in iteration 14 is 0.07788478510255971\n",
      "\n",
      "\n",
      "Error in iteration 15 is 0.09515605542842474\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.03787239265386609\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.07438635055465742\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.0685516583471454\n",
      "\n",
      "\n",
      "Error in iteration 19 is 0.06519849520554685\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.13201292908607326\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.12306236002860703\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.06244828054159084\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.12378160588257624\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.14873586949861825\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.06651870264125496\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.034173392029870554\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.05659999164476594\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.10588651002351789\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.07742119797875571\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.11836399505882886\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.15492439707509215\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.14506339333069349\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.09267039862897287\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.06137642745123811\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in iteration 35 is 0.08579030808359242\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.039239445071466844\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.03527675898889235\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.039564739825373244\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.04158497990164971\n",
      "\n",
      "\n",
      "Error in iteration 40 is 0.028825535866630933\n",
      "\n",
      "\n",
      "Error in iteration 41 is 0.03679545013514565\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.0\n",
      "\n",
      "\n",
      "K means on Sample with class label 6\n",
      "\n",
      "Error during first farther point initialization is 132.495282934903\n",
      "\n",
      "\n",
      "Error in iteration 1 is 123.16377330761757\n",
      "\n",
      "\n",
      "Error in iteration 2 is 20.796181859888968\n",
      "\n",
      "\n",
      "Error in iteration 3 is 9.98298472533129\n",
      "\n",
      "\n",
      "Error in iteration 4 is 5.625053936513202\n",
      "\n",
      "\n",
      "Error in iteration 5 is 3.3110471725758654\n",
      "\n",
      "\n",
      "Error in iteration 6 is 2.2415955005588883\n",
      "\n",
      "\n",
      "Error in iteration 7 is 1.4399081451843176\n",
      "\n",
      "\n",
      "Error in iteration 8 is 1.200429699915984\n",
      "\n",
      "\n",
      "Error in iteration 9 is 0.9130350624059272\n",
      "\n",
      "\n",
      "Error in iteration 10 is 0.7660277035869871\n",
      "\n",
      "\n",
      "Error in iteration 11 is 0.8100172032808788\n",
      "\n",
      "\n",
      "Error in iteration 12 is 0.7043272793644352\n",
      "\n",
      "\n",
      "Error in iteration 13 is 0.6526776895228431\n",
      "\n",
      "\n",
      "Error in iteration 14 is 0.5092201425579946\n",
      "\n",
      "\n",
      "Error in iteration 15 is 0.48348678815535295\n",
      "\n",
      "\n",
      "Error in iteration 16 is 0.40873081022587665\n",
      "\n",
      "\n",
      "Error in iteration 17 is 0.4646465813167559\n",
      "\n",
      "\n",
      "Error in iteration 18 is 0.44364945031495967\n",
      "\n",
      "\n",
      "Error in iteration 19 is 0.4471214551563846\n",
      "\n",
      "\n",
      "Error in iteration 20 is 0.399497075770404\n",
      "\n",
      "\n",
      "Error in iteration 21 is 0.3157549508305163\n",
      "\n",
      "\n",
      "Error in iteration 22 is 0.27667334812838656\n",
      "\n",
      "\n",
      "Error in iteration 23 is 0.24455131407549546\n",
      "\n",
      "\n",
      "Error in iteration 24 is 0.3481720475559989\n",
      "\n",
      "\n",
      "Error in iteration 25 is 0.29359270445693086\n",
      "\n",
      "\n",
      "Error in iteration 26 is 0.2365472420290951\n",
      "\n",
      "\n",
      "Error in iteration 27 is 0.22499362253749083\n",
      "\n",
      "\n",
      "Error in iteration 28 is 0.14681077358966996\n",
      "\n",
      "\n",
      "Error in iteration 29 is 0.162009678032096\n",
      "\n",
      "\n",
      "Error in iteration 30 is 0.13150469750859228\n",
      "\n",
      "\n",
      "Error in iteration 31 is 0.12850786485829715\n",
      "\n",
      "\n",
      "Error in iteration 32 is 0.04246351556362417\n",
      "\n",
      "\n",
      "Error in iteration 33 is 0.0506625601109565\n",
      "\n",
      "\n",
      "Error in iteration 34 is 0.0415580076607325\n",
      "\n",
      "\n",
      "Error in iteration 35 is 0.11701600149105566\n",
      "\n",
      "\n",
      "Error in iteration 36 is 0.10373685109231673\n",
      "\n",
      "\n",
      "Error in iteration 37 is 0.20745295884542483\n",
      "\n",
      "\n",
      "Error in iteration 38 is 0.19309275219684402\n",
      "\n",
      "\n",
      "Error in iteration 39 is 0.07969292563984214\n",
      "\n",
      "\n",
      "Error in iteration 40 is 0.06636522956783648\n",
      "\n",
      "\n",
      "Error in iteration 41 is 0.07672616635169388\n",
      "\n",
      "\n",
      "Error in iteration 42 is 0.06672117280929463\n",
      "\n",
      "\n",
      "Error in iteration 43 is 0.031320872486581136\n",
      "\n",
      "\n",
      "Error in iteration 44 is 0.027303108876700104\n",
      "\n",
      "\n",
      "Error in iteration 45 is 0.024618057797142708\n",
      "\n",
      "\n",
      "Error in iteration 46 is 0.02883349178859101\n",
      "\n",
      "\n",
      "Error in iteration 47 is 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>datapoints_cnt</th>\n",
       "      <th>iters_to_converge</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>470</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>429</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>203</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>457</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>376</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>441</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>559</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>878</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>669</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>673</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>829</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>887</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>283</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>177</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>676</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>191</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>617</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>591</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>618</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>519</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>548</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>227</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>792</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>197</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>184</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>664</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>736</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>434</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1028</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>152</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>565</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>300</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>911</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>650</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>457</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>381</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>266</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>281</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>388</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>481</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>409</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>425</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>335</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>867</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>287</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>172</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>928</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>388</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>318</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>277</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>419</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>281</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>153</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>809</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>602</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>354</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>527</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>154</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>484</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>558</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>770</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>751</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>214</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>553</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>585</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>609</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>605</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>113</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>253</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>385</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>746</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>425</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_number  datapoints_cnt  iters_to_converge  class_label\n",
       "0       0               0             300                 36            1\n",
       "1       1               1             393                 36            1\n",
       "2       2               2             504                 36            1\n",
       "3       3               3             470                 36            1\n",
       "4       4               4             429                 36            1\n",
       "5       5               5             203                 36            1\n",
       "6       6               6             457                 36            1\n",
       "7       7               7             376                 36            1\n",
       "8       8               8             441                 36            1\n",
       "9       9               9             559                 36            1\n",
       "10      0               0             544                 38            0\n",
       "11      1               1             206                 38            0\n",
       "12      2               2             878                 38            0\n",
       "13      3               3             800                 38            0\n",
       "14      4               4             669                 38            0\n",
       "15      5               5             673                 38            0\n",
       "16      6               6              75                 38            0\n",
       "17      7               7               9                 38            0\n",
       "18      8               8             829                 38            0\n",
       "19      9               9               1                 38            0\n",
       "20      0               0             182                 28            4\n",
       "21      1               1             549                 28            4\n",
       "22      2               2             887                 28            4\n",
       "23      3               3             283                 28            4\n",
       "24      4               4             177                 28            4\n",
       "25      5               5             198                 28            4\n",
       "26      6               6             676                 28            4\n",
       "27      7               7             191                 28            4\n",
       "28      8               8             617                 28            4\n",
       "29      9               9             591                 28            4\n",
       "30      0               0             359                 91            7\n",
       "31      1               1             618                 91            7\n",
       "32      2               2             519                 91            7\n",
       "33      3               3             548                 91            7\n",
       "34      4               4             227                 91            7\n",
       "35      5               5             792                 91            7\n",
       "36      6               6             197                 91            7\n",
       "37      7               7              80                 91            7\n",
       "38      8               8             184                 91            7\n",
       "39      9               9             664                 91            7\n",
       "40      0               0             736                 52            3\n",
       "41      1               1             434                 52            3\n",
       "42      2               2             106                 52            3\n",
       "43      3               3            1028                 52            3\n",
       "44      4               4             122                 52            3\n",
       "45      5               5             152                 52            3\n",
       "46      6               6             565                 52            3\n",
       "47      7               7             300                 52            3\n",
       "48      8               8              47                 52            3\n",
       "49      9               9             911                 52            3\n",
       "50      0               0             310                 61            5\n",
       "51      1               1             172                 61            5\n",
       "52      2               2             650                 61            5\n",
       "53      3               3             457                 61            5\n",
       "54      4               4             381                 61            5\n",
       "55      5               5             266                 61            5\n",
       "56      6               6             281                 61            5\n",
       "57      7               7             388                 61            5\n",
       "58      8               8             481                 61            5\n",
       "59      9               9             409                 61            5\n",
       "60      0               0             425                 60            8\n",
       "61      1               1             180                 60            8\n",
       "62      2               2             335                 60            8\n",
       "63      3               3             867                 60            8\n",
       "64      4               4             287                 60            8\n",
       "65      5               5             172                 60            8\n",
       "66      6               6             928                 60            8\n",
       "67      7               7             388                 60            8\n",
       "68      8               8             318                 60            8\n",
       "69      9               9             277                 60            8\n",
       "70      0               0             419                121            9\n",
       "71      1               1             281                121            9\n",
       "72      2               2             800                121            9\n",
       "73      3               3             153                121            9\n",
       "74      4               4             809                121            9\n",
       "75      5               5             602                121            9\n",
       "76      6               6             354                121            9\n",
       "77      7               7              38                121            9\n",
       "78      8               8             527                121            9\n",
       "79      9               9             154                121            9\n",
       "80      0               0             484                 42            2\n",
       "81      1               1             558                 42            2\n",
       "82      2               2             770                 42            2\n",
       "83      3               3             751                 42            2\n",
       "84      4               4             214                 42            2\n",
       "85      5               5             262                 42            2\n",
       "86      6               6             553                 42            2\n",
       "87      7               7              57                 42            2\n",
       "88      8               8             362                 42            2\n",
       "89      9               9              61                 42            2\n",
       "90      0               0             585                 47            6\n",
       "91      1               1             158                 47            6\n",
       "92      2               2             184                 47            6\n",
       "93      3               3             609                 47            6\n",
       "94      4               4             605                 47            6\n",
       "95      5               5             113                 47            6\n",
       "96      6               6             253                 47            6\n",
       "97      7               7             385                 47            6\n",
       "98      8               8             746                 47            6\n",
       "99      9               9             425                 47            6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create kmeansobj instance \n",
    "kmeansobj=PCA(PCA_30, 30)  # label column position in PCA_30 is 30th\n",
    "unq_labels = kmeansobj.labels # 10 unique labels in the digits data\n",
    "\n",
    "# #iteration on class label \n",
    "# subclass, cls_label = kmeansobj.subset_data(0)  #subset data with label = 0 \n",
    "# batch = subclass.values # batch has shape N*30. dimensions = 30 (after PCA).\n",
    "\n",
    "#define global parameters\n",
    "K = 10 # number of clusters as per user requirement (user defined)\n",
    "# D = batch.shape[1]  # dimensions = 30 \n",
    "# N = batch.shape[0]  # sample datapoints = N \n",
    "output_summary = pd.DataFrame()\n",
    "for cls_labelname in unq_labels:\n",
    "    print(\"\\nK means on Sample with class label {}\".format(cls_labelname))\n",
    "    subclass, cls_label = kmeansobj.subset_data(cls_labelname)  #subset data with label = 0 \n",
    "    batch = subclass.values # batch has shape N*30. dimensions = 30 (after PCA).\n",
    "    D = batch.shape[1]  # dimensions = 30 \n",
    "    N = batch.shape[0]  # sample datapoints = N \n",
    "    outp=kmeans(K, D, N, batch, cls_labelname)\n",
    "    output_summary=pd.concat([output_summary,outp],axis=0)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "output_summary=output_summary.reset_index()\n",
    "\n",
    "output_summary.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
